{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923256bad89ddad3",
   "metadata": {},
   "source": [
    "# DSAIT4335 Recommender Systems\n",
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf65ba37274efd8",
   "metadata": {},
   "source": [
    "In this project, you will work to build different recommendation models and evaluate the effectiveness of these models through offline experiments. The dataset used for the experiments is **MovieLens100K**, a movie recommendation dataset collected by GroupLens: https://grouplens.org/datasets/movielens/100k/. For more details, check the project description on Brightspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b294067d0ceb829",
   "metadata": {},
   "source": [
    "# Instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c14ed1d5bdf00",
   "metadata": {},
   "source": [
    "The MovieLens100K is already splitted into 80% training and 20% test sets. Along with training and test sets, movies metadata as content information is also provided.\n",
    "\n",
    "**Expected file structure** for this assignment:   \n",
    "   \n",
    "   ```\n",
    "   RecSysProject/\n",
    "   ‚îú‚îÄ‚îÄ training.txt\n",
    "   ‚îú‚îÄ‚îÄ test.txt\n",
    "   ‚îú‚îÄ‚îÄ movies.txt\n",
    "   ‚îî‚îÄ‚îÄ codes.ipynb\n",
    "   ```\n",
    "\n",
    "**Note:** Be sure to run all cells in each section sequentially, so that intermediate variables and packages are properly carried over to subsequent cells.\n",
    "\n",
    "**Note** Be sure to run all cells such that the submitted file contains the output of each cell.\n",
    "\n",
    "**Note** Feel free to add cells if you need more for answering a question.\n",
    "\n",
    "**Submission:** Answer all the questions in this jupyter-notebook file. Submit this jupyter-notebook file (your answers included) to Brightspace. Change the name of this jupyter-notebook file to your group number: example, group10 -> 10.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977945fa-a202-49c4-a41d-12ada7b437da",
   "metadata": {
    "id": "977945fa-a202-49c4-a41d-12ada7b437da"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302a2b5b-fdf1-41c8-b6a6-bc1cd453425b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:05.856205Z",
     "start_time": "2025-10-24T12:45:01.856573Z"
    },
    "executionInfo": {
     "elapsed": 16235,
     "status": "ok",
     "timestamp": 1761075925663,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "302a2b5b-fdf1-41c8-b6a6-bc1cd453425b"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers torch  # For BERT\n",
    "!pip install numpy pandas matplotlib seaborn scipy scikit-learn torch tqdm\n",
    "!pip install torch-directml\n",
    "\n",
    "# you can refer https://huggingface.co/docs/transformers/en/model_doc/bert for various versions of the pre-trained model BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3b0e3c-74d9-436b-b676-56bc2a8528a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:11.431960Z",
     "start_time": "2025-10-24T12:45:05.873003Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1761075925694,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "0e3b0e3c-74d9-436b-b676-56bc2a8528a9",
    "outputId": "2f83e57e-792e-4b09-8b94-8ed28d0d334e"
   },
   "outputs": [],
   "source": [
    "# For BERT embeddings (install: pip install transformers torch)\n",
    "print(\"Check the status of BERT installation:\")\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    import torch\n",
    "\n",
    "    BERT_AVAILABLE = True\n",
    "    directml_available = False\n",
    "    device = None\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        # Try DirectML (Windows / AMD) if installed\n",
    "        try:\n",
    "            import torch_directml\n",
    "            dml_device = torch_directml.device()\n",
    "            for i in range(torch_directml.device_count()):\n",
    "                print(f\"  Device {i}: {torch_directml.device_name(i)}\")\n",
    "            device = dml_device\n",
    "            directml_available = True\n",
    "        except Exception:\n",
    "            # Try Apple MPS\n",
    "            if getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available():\n",
    "                device = torch.device(\"mps\")\n",
    "            else:\n",
    "                # Fallback to CPU\n",
    "                device = torch.device(\"cpu\")\n",
    "\n",
    "    print(\"BERT libraries loaded successfully!\")\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "    print(\"torch version:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available(), \" CUDA devices:\", torch.cuda.device_count() if torch.cuda.is_available() else 0)\n",
    "    print(\"MPS available:\", getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available())\n",
    "    print(\"DirectML available:\", directml_available)\n",
    "    print(\"Using device:\", device)\n",
    "except ImportError:\n",
    "    BERT_AVAILABLE = False\n",
    "    import torch\n",
    "    device = torch.device(\"cpu\")\n",
    "    directml_available = False\n",
    "    print(\"BERT libraries not available. Install with: pip install transformers torch\")\n",
    "    print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8055513b-9f14-4d18-b32a-7c2ee386e6e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:11.936879Z",
     "start_time": "2025-10-24T12:45:11.896340Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1761075925700,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "8055513b-9f14-4d18-b32a-7c2ee386e6e3",
    "outputId": "1d4104d8-9733-4c2e-a3ea-c6986b497b23"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import cosine, correlation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import time, math\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import itertools\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b83b831",
   "metadata": {},
   "source": [
    "### Here we set the random seed to 10 in order to ensure reproducibility of the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VFxfSxh-Oztd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:12.096523Z",
     "start_time": "2025-10-24T12:45:12.071118Z"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1761075925702,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "VFxfSxh-Oztd"
   },
   "outputs": [],
   "source": [
    "SEED = 10\n",
    "\n",
    "# Python built-in random seed\n",
    "random.seed(SEED)\n",
    "\n",
    "# Numpy random seed\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# PyTorch random seed (CPU)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# PyTorch random seed (GPU, if available)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  # if using multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa06aa-00e8-4e4e-8ec9-907dffe0caf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:12.112150Z",
     "start_time": "2025-10-24T12:45:12.106605Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compatibility for older numpy version\n",
    "try:\n",
    "    import numpy.core.numeric\n",
    "    import sys\n",
    "    sys.modules['numpy._core.numeric'] = numpy.core.numeric\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKb-8I4DG1Pl",
   "metadata": {
    "id": "PKb-8I4DG1Pl"
   },
   "source": [
    "## Mount Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qdqyKhLeG08Z",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:12.143287Z",
     "start_time": "2025-10-24T12:45:12.124185Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1602,
     "status": "ok",
     "timestamp": 1761075927304,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "qdqyKhLeG08Z",
    "outputId": "bd28c4e8-8e6a-4e8c-cc01-328534a93a28"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Use current directory as project root\n",
    "PROJECT_PATH = os.getcwd()  # e.g. /Users/yourname/Documents/Project-RecSys\n",
    "OUTPUTS_PATH = os.path.join(PROJECT_PATH, \"data\")\n",
    "\n",
    "# Make sure 'data' folder exists\n",
    "os.makedirs(OUTPUTS_PATH, exist_ok=True)\n",
    "\n",
    "print(\"Project Path:\", PROJECT_PATH)\n",
    "print(\"Data Path:\", OUTPUTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HbPQYABRH2sW",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:12.205897Z",
     "start_time": "2025-10-24T12:45:12.189308Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1761075927345,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "HbPQYABRH2sW",
    "outputId": "b2f62e0a-b71b-4d94-ac4e-42bb41853549"
   },
   "outputs": [],
   "source": [
    "%cd $PROJECT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbd1eff-8e8b-4f65-b92a-778107a256cc",
   "metadata": {
    "id": "fbbd1eff-8e8b-4f65-b92a-778107a256cc"
   },
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d220e9dc-3a45-4d25-b214-23d6555cb34d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:12.393175Z",
     "start_time": "2025-10-24T12:45:12.286427Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 723
    },
    "executionInfo": {
     "elapsed": 983,
     "status": "ok",
     "timestamp": 1761075928330,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "d220e9dc-3a45-4d25-b214-23d6555cb34d",
    "outputId": "fe25085a-266c-4a53-d518-7c2c3975c242"
   },
   "outputs": [],
   "source": [
    "# loading the training set and test set\n",
    "columns_name=['user_id','item_id','rating','timestamp']\n",
    "train_data = pd.read_csv(f'{PROJECT_PATH}/training.txt', sep='\\t', names=columns_name)\n",
    "test_data = pd.read_csv(f'{PROJECT_PATH}/test.txt', sep='\\t', names=columns_name)\n",
    "\n",
    "# We take 20% of the original training set as validation\n",
    "train_data, val_data = train_test_split(\n",
    "    train_data,\n",
    "    test_size=0.2,         # 20% validation\n",
    "    random_state=10,       # ensure reproducibility\n",
    "    shuffle=True           # shuffle before splitting\n",
    ")\n",
    "\n",
    "print(f'The training data:')\n",
    "display(train_data[['user_id','item_id','rating']].head())\n",
    "print(f'The shape of the training data: {train_data.shape}')\n",
    "print('--------------------------------')\n",
    "print(f'The validation data:')\n",
    "display(val_data[['user_id','item_id','rating']].head())\n",
    "print(f'The shape of the validation data: {val_data.shape}')\n",
    "print('--------------------------------')\n",
    "print(f'The test data:')\n",
    "display(test_data[['user_id','item_id','rating']].head())\n",
    "print(f'The shape of the test data: {test_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e84160d-ef0e-4e58-8ace-307fb8cd5a79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:12.734160Z",
     "start_time": "2025-10-24T12:45:12.622441Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1761075929075,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "1e84160d-ef0e-4e58-8ace-307fb8cd5a79",
    "outputId": "22bc8616-1069-4cd1-e1dd-1a2d27572027"
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv(f'{PROJECT_PATH}/movies.txt',names=['item_id','title','genres','description'],sep='\\t')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mHVb4DYUG3oN",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:15.881642Z",
     "start_time": "2025-10-24T12:45:15.874170Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1761075929083,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "mHVb4DYUG3oN",
    "outputId": "57d3585d-5f95-4666-f5e2-755cdc59823d"
   },
   "outputs": [],
   "source": [
    "num_users = train_data['user_id'].max()\n",
    "num_items = train_data['item_id'].max()\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Number of users: {num_users}, Number of items: {num_items}, Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aEJbEr7PRARh",
   "metadata": {
    "id": "aEJbEr7PRARh"
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yNqWrIq35fZ5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:16.255888Z",
     "start_time": "2025-10-24T12:45:16.236475Z"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1761075929115,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "yNqWrIq35fZ5"
   },
   "outputs": [],
   "source": [
    "# Create embeddings of the movie items\n",
    "def create_bert_embeddings(content):\n",
    "    \"\"\"\n",
    "    Generate BERT embeddings for movie content.\n",
    "\n",
    "    Args:\n",
    "        content: Content of items\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: BERT embeddings matrix\n",
    "    \"\"\"\n",
    "    if not BERT_AVAILABLE:\n",
    "        print(\"BERT libraries not available. Install with: pip install transformers torch\")\n",
    "        return None\n",
    "\n",
    "    if content is None:\n",
    "        return None\n",
    "\n",
    "    if isinstance(content, pd.Series):\n",
    "        content = content.fillna(\"\").astype(str).tolist()\n",
    "    elif isinstance(content, np.ndarray):\n",
    "        content = content.astype(str).tolist()\n",
    "\n",
    "    model_name = 'distilbert-base-uncased'\n",
    "\n",
    "    print(f\"Loading BERT model: {model_name}\")\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    # Set device (GPU if available)\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using: {device}\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Generate embeddings in batches\n",
    "    batch_size = 32  # Adjust based on available memory\n",
    "    emb = []\n",
    "\n",
    "    for i in range(0, len(content), batch_size):\n",
    "        if i % (batch_size * 10) == 0:\n",
    "            print(f\"Processing batch {i//batch_size + 1}/{len(content)//batch_size + 1}\")\n",
    "\n",
    "        batch_texts = content[i:i + batch_size]\n",
    "\n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Move to device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            # Use [CLS] token embedding (first token)\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            emb.extend(batch_embeddings)\n",
    "\n",
    "    emb = np.array(emb)\n",
    "\n",
    "    print(f\"BERT embeddings generated: {emb.shape}\")\n",
    "    print(f\"Embedding dimension: {emb.shape[1]}\")\n",
    "\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UMSyrb1wSIzX",
   "metadata": {
    "id": "UMSyrb1wSIzX"
   },
   "source": [
    "## Get the item embeddings and then save them to local folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2hJEthiyQjbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:16.410310Z",
     "start_time": "2025-10-24T12:45:16.349325Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2071,
     "status": "ok",
     "timestamp": 1761075931206,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "2hJEthiyQjbc",
    "outputId": "5aaeb8d0-0181-4bf2-c2cc-a4fca3245003"
   },
   "outputs": [],
   "source": [
    "# Three diffrent content representation (title + genre, description, title + genre + description)\n",
    "\n",
    "import os\n",
    "# Create a subfolder for saved outputs to avoid performing expensive computations multiple times\n",
    "os.makedirs(OUTPUTS_PATH, exist_ok=True)\n",
    "\n",
    "# Define the file paths for each of the three embedding arrays\n",
    "path_titlegenres = os.path.join(OUTPUTS_PATH, \"item_emb_titlegenres.npy\")\n",
    "path_description = os.path.join(OUTPUTS_PATH, \"item_emb_description.npy\")\n",
    "path_full = os.path.join(OUTPUTS_PATH, \"item_emb_full.npy\")\n",
    "\n",
    "if os.path.exists(path_titlegenres):\n",
    "    print(f\"Loading Title+Genres embeddings from: {path_titlegenres} üíæ\")\n",
    "    item_emb_titlegenres = np.load(path_titlegenres)\n",
    "    print(\"‚úÖ Loaded successfully!\")\n",
    "else:\n",
    "    print(\"Computing Title+Genres embeddings... ‚è≥\")\n",
    "    title_genres = movies[\"title\"] + ' ' + movies[\"genres\"]\n",
    "    item_emb_titlegenres = create_bert_embeddings(title_genres)\n",
    "    np.save(path_titlegenres, item_emb_titlegenres)\n",
    "    print(f\"‚úÖ Computed and saved to shared drive for future use!\")\n",
    "\n",
    "if os.path.exists(path_description):\n",
    "    print(f\"\\nLoading Description embeddings from: {path_description} üíæ\")\n",
    "    item_emb_description = np.load(path_description)\n",
    "    print(\"‚úÖ Loaded successfully!\")\n",
    "else:\n",
    "    print(\"\\nComputing Description embeddings... ‚è≥\")\n",
    "    item_emb_description = create_bert_embeddings(movies[\"description\"])\n",
    "    np.save(path_description, item_emb_description)\n",
    "    print(f\"‚úÖ Computed and saved to shared drive for future use!\")\n",
    "\n",
    "if os.path.exists(path_full):\n",
    "    print(f\"\\nLoading Full Content embeddings from: {path_full} üíæ\")\n",
    "    item_emb_full = np.load(path_full)\n",
    "    print(\"‚úÖ Loaded successfully!\")\n",
    "else:\n",
    "    print(\"\\nComputing Full Content embeddings... ‚è≥\")\n",
    "    title_genres_description = movies[\"title\"] + ' ' + movies[\"genres\"] + ' ' + movies[\"description\"]\n",
    "    item_emb_full = create_bert_embeddings(title_genres_description)\n",
    "    np.save(path_full, item_emb_full)\n",
    "    print(f\"‚úÖ Computed and saved to shared drive for future use!\")\n",
    "\n",
    "print(\"\\n--- Final Embedding Shapes ---\")\n",
    "print(f\"Title+Genres: {item_emb_titlegenres.shape}\")\n",
    "print(f\"Description:  {item_emb_description.shape}\")\n",
    "print(f\"Full Content: {item_emb_full.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adeb076",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:16.518366Z",
     "start_time": "2025-10-24T12:45:16.512164Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get item embedding based on item ID (item id starts at 1)\n",
    "def get_item_emb(item_id, content_type):\n",
    "    emb = None\n",
    "\n",
    "    if content_type == \"title_genres\":\n",
    "        emb = item_emb_titlegenres[item_id - 1]\n",
    "    elif content_type == \"description\":\n",
    "        emb = item_emb_description[item_id - 1]\n",
    "    elif content_type == \"full\":\n",
    "        emb = item_emb_full[item_id - 1]\n",
    "\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KZiVQ-F1SS72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:16.564546Z",
     "start_time": "2025-10-24T12:45:16.551028Z"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1761075931209,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "KZiVQ-F1SS72"
   },
   "outputs": [],
   "source": [
    "# Given content type (title+genres, description, or title+genres+description) returns the embeddings and ratings of interacted items by user_id\n",
    "def get_interacted_items_embs_rating(train_data, user_id, content_type):\n",
    "    embs, ratings = [], []\n",
    "\n",
    "    # use train_data to retrieve the item_ids that target user interacted\n",
    "    items_and_ratings = train_data[train_data[\"user_id\"] == user_id][['item_id', 'rating']]\n",
    "    item_ids = items_and_ratings['item_id']\n",
    "    ratings = items_and_ratings['rating'].tolist()\n",
    "\n",
    "    indices = [i - 1 for i in item_ids]\n",
    "\n",
    "    if content_type == \"title_genres\":\n",
    "        embs = item_emb_titlegenres[indices]\n",
    "    elif content_type == \"description\":\n",
    "        embs = item_emb_description[indices]\n",
    "    elif content_type == \"full\":\n",
    "        embs = item_emb_full[indices]\n",
    "\n",
    "    return embs, ratings\n",
    "\n",
    "# given content type (title+genres, description, or title+genres+description) and aggregation method (avg, weighted_avg, avg_pos) returns the representation of a user.\n",
    "def get_user_emb(train_data, user_id, content_type, aggregation_method):\n",
    "\n",
    "    emb = []\n",
    "\n",
    "    # obtain item embeddings and corresponding ratings of the items that the user has interacted with\n",
    "    item_embs, ratings = get_interacted_items_embs_rating(\n",
    "        train_data, user_id, content_type\n",
    "    )\n",
    "\n",
    "    interacted_ratings = np.array(ratings)\n",
    "\n",
    "    if aggregation_method == 'avg':\n",
    "        emb = np.mean(item_embs, axis=0)\n",
    "\n",
    "    elif aggregation_method == 'weighted_avg':\n",
    "        emb = np.average(item_embs, axis=0, weights=interacted_ratings)\n",
    "\n",
    "    elif aggregation_method == 'avg_pos':\n",
    "        ratings = np.array(ratings)\n",
    "\n",
    "        # Filter for only positively rated items above 4\n",
    "        positive_mask = ratings >= 4\n",
    "        positive_embs = item_embs[positive_mask]\n",
    "\n",
    "        if len(positive_embs) > 0:\n",
    "            emb = np.mean(positive_embs, axis=0)\n",
    "        else:\n",
    "            emb = np.zeros(item_embs.shape[1])\n",
    "\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BTNETH4r9rHM",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:16.596017Z",
     "start_time": "2025-10-24T12:45:16.579542Z"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1761075931225,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "BTNETH4r9rHM"
   },
   "outputs": [],
   "source": [
    "def RMSE(actual_rating, pred_rating):\n",
    "    # Implement a function that computes RMSE error between actual ratings and predicted ratings.\n",
    "    # Note that actual_ratings and pred_ratings are lists.\n",
    "\n",
    "    result = 0.0\n",
    "\n",
    "    ############# Your code here ############\n",
    "    result = np.mean((np.array(pred_rating) - np.array(actual_rating))**2)**(1/2)\n",
    "    #########################################\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d50b57f-b07a-49b0-ad8c-31566a355cc7",
   "metadata": {
    "id": "1d50b57f-b07a-49b0-ad8c-31566a355cc7"
   },
   "source": [
    "# Task 1) Implementation of different recommendation models as well as a hybrid model combining those recommendation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q03p6xVQJajO",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:16.611255Z",
     "start_time": "2025-10-24T12:45:16.606572Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1761075931240,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "q03p6xVQJajO",
    "outputId": "1a86b99e-2add-44f8-bfbb-9c119023ddfd"
   },
   "outputs": [],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5N4IVgQQQTqh",
   "metadata": {
    "id": "5N4IVgQQQTqh"
   },
   "source": [
    "## 1.1 Content Based Recommendation model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fUL-_XFEQORn",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:16.673181Z",
     "start_time": "2025-10-24T12:45:16.661430Z"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1761075931281,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "fUL-_XFEQORn"
   },
   "outputs": [],
   "source": [
    "# compute the predicted rating for a user-item pair\n",
    "def get_user_item_prediction_content_based(train_data, user_id, item_id, content_type, aggregation_method):\n",
    "    pred_rating = 0.0\n",
    "\n",
    "    user_embedding = get_user_emb(train_data, user_id, content_type, aggregation_method)\n",
    "    item_embedding = get_item_emb(item_id, content_type)\n",
    "\n",
    "    pred_rating = np.dot(user_embedding, item_embedding)\n",
    "\n",
    "    return pred_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e25ab-353d-4a7c-bf68-9deb201bfbb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:16.689134Z",
     "start_time": "2025-10-24T12:45:16.683417Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1761075931282,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "f55e25ab-353d-4a7c-bf68-9deb201bfbb6",
    "outputId": "c18dc90f-0a59-4ff1-8aa9-9928c8e73bd2"
   },
   "outputs": [],
   "source": [
    "user_id, item_id = 100, 266\n",
    "content_type, aggregation_method = 'full', 'avg' # alternatives are content_type={title_genres,description,full} and aggregation_method={avg,weighted_avg,avg_pos}\n",
    "print('Predicted score for user_id=100 and item_id=266 for content type '+content_type+' and aggregation method '+aggregation_method+':')\n",
    "print(get_user_item_prediction_content_based(train_data, user_id, item_id, content_type, aggregation_method))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa3f82e",
   "metadata": {},
   "source": [
    "### Function that generates top-k recommendation list for a target user with user-based recommendataion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3244e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:17.895827Z",
     "start_time": "2025-10-24T12:45:16.740229Z"
    }
   },
   "outputs": [],
   "source": [
    "def recommend_topk_content_based(train_data, target_user, k, content_type, aggregation_method):\n",
    "    \"\"\"\n",
    "    Generate Top-K recommendations for a target user using Content-based CF.\n",
    "\n",
    "    Args:\n",
    "        train_data (pd.DataFrame): ratings data with columns [user_id, item_id, rating]\n",
    "        target_user (int): user_id of the target user\n",
    "        k (int): number of items to recommend\n",
    "\n",
    "    Returns:\n",
    "        list of (item_id, predicted_score) sorted by score desc\n",
    "    \"\"\"\n",
    "    result = []\n",
    "\n",
    "    user_rated_items = set(train_data[train_data[\"user_id\"] == target_user][\"item_id\"])\n",
    "    all_items = set(train_data[\"item_id\"].unique())\n",
    "    candidate_items = all_items - user_rated_items\n",
    "\n",
    "    predictions = []\n",
    "    for item_id in candidate_items:\n",
    "        pred_rating = get_user_item_prediction_content_based(\n",
    "            train_data,\n",
    "            target_user,\n",
    "            item_id,\n",
    "            content_type,\n",
    "            aggregation_method\n",
    "        )\n",
    "        if not np.isnan(pred_rating):  # only keep valid predictions\n",
    "            predictions.append((item_id, pred_rating))\n",
    "\n",
    "    ratings = [r for _, r in predictions]\n",
    "    min_rating, max_rating = min(ratings), max(ratings)\n",
    "\n",
    "    # print(f\"Min predicted rating: {min_rating}, Max predicted rating: {max_rating}\")\n",
    "    # print(predictions[:5])\n",
    "\n",
    "    if max_rating == min_rating:\n",
    "        mapped_ratings = [(3.0, i) for i, _ in predictions]\n",
    "    else:\n",
    "        mapped_ratings = [\n",
    "            (\n",
    "                i,\n",
    "                1 + (r - min_rating) * 4 / (max_rating - min_rating),\n",
    "            )\n",
    "            for i, r in predictions\n",
    "        ]\n",
    "\n",
    "    result = sorted(mapped_ratings, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "target_user, k = 1, 50\n",
    "recommendations = recommend_topk_content_based(train_data, target_user, k, content_type=\"full\", aggregation_method=\"weighted_avg\")\n",
    "print(\n",
    "    f\"Top-{k} recommendations for user {target_user} with content-based recommendation:\"\n",
    ")\n",
    "for item, score in recommendations:\n",
    "    print(f\"Item {item}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aiKjcrm8VANo",
   "metadata": {
    "id": "aiKjcrm8VANo"
   },
   "source": [
    "### Computing RMSE of the content-based recommendation model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lGGd3MSGVgGe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:17.973124Z",
     "start_time": "2025-10-24T12:45:17.962388Z"
    },
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1761075931283,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "lGGd3MSGVgGe"
   },
   "outputs": [],
   "source": [
    "def evaluate_rating_prediction_content_based(train_data, test_data, content_type, aggregation_method):\n",
    "    # Create a mapping from user_id to user embedding\n",
    "    users_emb = {}\n",
    "    users = list(train_data['user_id'].unique())\n",
    "    for user in tqdm(users, desc=\"Creating user embedding mapping\"):\n",
    "        users_emb[user] = get_user_emb(train_data, user, content_type, aggregation_method)\n",
    "\n",
    "\n",
    "    # I do the same for items\n",
    "    items = list(train_data['item_id'].unique())\n",
    "    items_emb = {}\n",
    "    for item in tqdm(items, desc=\"Creating item embedding mapping\"):\n",
    "        items_emb[item] = get_item_emb(item, content_type)\n",
    "\n",
    "    actual_ratings, pred_ratings = [], []\n",
    "\n",
    "    global_mean_rating = train_data['rating'].mean()\n",
    "\n",
    "    for index, test_row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Processing test data\"):\n",
    "        user_id = test_row[\"user_id\"]\n",
    "        item_id = test_row[\"item_id\"]\n",
    "\n",
    "        user_embedding = users_emb.get(user_id)\n",
    "        item_embedding = items_emb.get(item_id)\n",
    "\n",
    "        if user_embedding is None or item_embedding is None:\n",
    "            # Impute the global average rating if we can't predict\n",
    "            pred_rating = global_mean_rating\n",
    "        else:\n",
    "            # Only calculate dot product if we have both embeddings\n",
    "            pred_rating = np.dot(user_embedding, item_embedding)\n",
    "\n",
    "        actual_ratings.append(test_row['rating'])\n",
    "        pred_ratings.append(pred_rating)\n",
    "\n",
    "    # Given predicted ratings, map them into [1,5] interval using -> 1 + (pred - min_val) * (4 / (max_val - min_val))\n",
    "\n",
    "    min_rating = min(pred_ratings)\n",
    "    max_rating = max(pred_ratings)\n",
    "\n",
    "    mapped_ratings = [1 + (rating - min_rating) * 4 / (max_rating - min_rating) for rating in pred_ratings]\n",
    "\n",
    "\n",
    "    mae_value, mse_value, rmse_value = 0.0, 0.0, 0.0\n",
    "\n",
    "\n",
    "    # compute the metrics: MAE, MSE, RMSE\n",
    "    # mae_value = MAE(actual_ratings, mapped_ratings)\n",
    "    # mse_value = MSE(actual_ratings, mapped_ratings)\n",
    "    rmse_value = RMSE(actual_ratings, mapped_ratings)\n",
    "\n",
    "    return rmse_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VLYAsRiEXMsx",
   "metadata": {
    "id": "VLYAsRiEXMsx"
   },
   "source": [
    "### Use the RMSE loss to find out which combination of content representation and aggregation method works the best for content-recommendation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XRpPd6myXqsj",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:17.988948Z",
     "start_time": "2025-10-24T12:45:17.982221Z"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1761075931329,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "XRpPd6myXqsj"
   },
   "outputs": [],
   "source": [
    "def hyperparameter_search_content_based(train_data, val_data, content_types, aggregation_methods):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for content based recommendation. It finds the best combination\n",
    "    of content representations and aggregation methods\n",
    "\n",
    "    Args:\n",
    "        train_data (pd.DataFrame): training set with columns ['user_id', 'item_id', 'rating']\n",
    "        val_data (pd.DataFrame): validation set with columns ['user_id', 'item_id', 'rating']\n",
    "        content_types: content types list (title_genres, description, full)\n",
    "        aggregation_methods: avg, weighted_avg, avg_pos\n",
    "\n",
    "    Returns:\n",
    "        dict: {'best_config': (best_content_type, best_aggregation_method), 'best_rmse': float, 'results': list of tuples (k, rmse)}\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n=== Content Based Recommendation Hyperparameter Search ===\")\n",
    "    results = []\n",
    "\n",
    "    for content_type in content_types:\n",
    "        for agg_method in aggregation_methods:\n",
    "          print(f\"starting with {content_type} and {agg_method}\")\n",
    "          rmse = evaluate_rating_prediction_content_based(\n",
    "              train_data,\n",
    "              val_data,\n",
    "              content_type,\n",
    "              agg_method\n",
    "          )\n",
    "\n",
    "          results.append({\n",
    "              'content_type': content_type,\n",
    "              'aggregation_method': agg_method,\n",
    "              'RMSE': rmse\n",
    "          })\n",
    "          print(f\"Config: ({content_type}, {agg_method}) -> RMSE = {rmse:.4f}\")\n",
    "\n",
    "    # Get the best performing combinations as the result of this hyperparameter tuning\n",
    "    best_result = min(results, key=lambda x: x['RMSE'])\n",
    "    best_config = (best_result['content_type'], best_result['aggregation_method'])\n",
    "    best_rmse = best_result['RMSE']\n",
    "\n",
    "    print(f\"Best result was achieved by the config: {best_config}\")\n",
    "\n",
    "    return {\n",
    "        'best_config': best_config,\n",
    "        'best_rmse': best_rmse,\n",
    "        'results': results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OqXf2GZSbDZa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:18.019497Z",
     "start_time": "2025-10-24T12:45:17.999478Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52102,
     "status": "ok",
     "timestamp": 1761075983443,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "OqXf2GZSbDZa",
    "outputId": "7cdd3a7c-df24-49c7-be5b-85026cfe78c8"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Define parameter space\n",
    "content_types_grid = ['title_genres', 'description', 'full']\n",
    "aggregation_methods_grid = ['avg', 'weighted_avg', 'avg_pos']\n",
    "\n",
    "path_content_based_tuning = os.path.join(OUTPUTS_PATH, 'content_based_tuning_results.pkl')\n",
    "\n",
    "# Run the search or load results\n",
    "if os.path.exists(path_content_based_tuning):\n",
    "    print(f\"Loading Content-based tuning results from: {path_content_based_tuning} üíæ\")\n",
    "    with open(path_content_based_tuning, 'rb') as f:\n",
    "        content_based_tuning = pickle.load(f)\n",
    "    print(\"‚úÖ Loaded successfully!\")\n",
    "else:\n",
    "    print(\"Running Content-based hyperparameter search... ‚è≥\")\n",
    "\n",
    "    content_based_tuning = hyperparameter_search_content_based(\n",
    "        train_data,\n",
    "        val_data,\n",
    "        content_types_grid,\n",
    "        aggregation_methods_grid\n",
    "    )\n",
    "\n",
    "    # Save the results\n",
    "    with open(path_content_based_tuning, 'wb') as f:\n",
    "        pickle.dump(content_based_tuning, f)\n",
    "    print(f\"‚úÖ Saved Content-based tuning results to {path_content_based_tuning}\")\n",
    "\n",
    "print(\"\\n--- Content-Based Tuning Results ---\")\n",
    "print(f\"Best Config: {content_based_tuning['best_config']}\")\n",
    "print(f\"Best Validation RMSE: {content_based_tuning['best_rmse']:.4f}\")\n",
    "\n",
    "rmse_content_based = content_based_tuning['best_rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gFFuU1JCuM_V",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:18.311703Z",
     "start_time": "2025-10-24T12:45:18.064792Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 860,
     "status": "ok",
     "timestamp": 1761075984304,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "gFFuU1JCuM_V",
    "outputId": "9df189ae-277e-414b-8b62-a4d4482fc274"
   },
   "outputs": [],
   "source": [
    "# Visualize Content based recommendation hyperparameter tuning results\n",
    "\n",
    "results_df = pd.DataFrame(content_based_tuning['results'], columns=['content_type', \"aggregation_method\", 'RMSE'])\n",
    "results_df[\"cobination\"] = results_df[\"content_type\"] + \" + \" + results_df[\"aggregation_method\"]\n",
    "results_df = results_df.sort_values('RMSE')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(results_df['cobination'], results_df['RMSE'], color='skyblue', edgecolor='black')\n",
    "\n",
    "plt.title(\"Content-based CF Hyperparameter Tuning\\n(RMSE by content_type + aggregation_method)\", fontsize=12)\n",
    "plt.xlabel(\"content_type + aggregation_method combination\", fontsize=10)\n",
    "plt.ylabel(\"Validation RMSE\", fontsize=10)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w63PCaiNr5e9",
   "metadata": {
    "id": "w63PCaiNr5e9"
   },
   "source": [
    "## 1.2 User-based neighborhood method (UserKNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UR9oe4EpmHuZ",
   "metadata": {
    "id": "UR9oe4EpmHuZ"
   },
   "source": [
    "The **Pearson correlation coefficient** between two users \\(x\\) and \\(y\\) is defined as:\n",
    "\n",
    "$$\n",
    "r_{xy} = \\frac{\\sum_{i \\in I_{xy}} (x_i - \\bar{x})(y_i - \\bar{y})}\n",
    "              {\\sqrt{\\sum_{i \\in I_{xy}} (x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum_{i \\in I_{xy}} (y_i - \\bar{y})^2}}\n",
    "$$\n",
    "\n",
    "**Where:**\n",
    "\n",
    "- $I_{xy}$ = set of items rated by both users  \n",
    "- $x_i$, $y_i$ = ratings of users \\(x\\) and \\(y\\) on item \\(i\\)  \n",
    "- $\\bar{x}$, $\\bar{y}$ = mean ratings of users \\(x\\) and \\(y\\) on the common items  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GHv4hw8wACh5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:18.342071Z",
     "start_time": "2025-10-24T12:45:18.331330Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1761075984308,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "GHv4hw8wACh5"
   },
   "outputs": [],
   "source": [
    "# define pearson correlation to obtain users similarity\n",
    "def pearson_correlation(user1_ratings: pd.Series, user2_ratings: pd.Series) -> float:\n",
    "    # only consider items that have been rated by both users\n",
    "    common_items = user1_ratings.dropna().index.intersection(user2_ratings.dropna().index)\n",
    "\n",
    "    if len(common_items) < 2:\n",
    "        return 0\n",
    "\n",
    "    u1_ratings = user1_ratings.loc[common_items]\n",
    "    u2_ratings = user2_ratings.loc[common_items]\n",
    "\n",
    "    u1_mean = u1_ratings.mean()\n",
    "    u2_mean = u2_ratings.mean()\n",
    "\n",
    "    numerator = ((u1_ratings - u1_mean) * (u2_ratings - u2_mean)).sum()\n",
    "    denominator = np.sqrt(((u1_ratings - u1_mean) ** 2).sum()) * np.sqrt(((u2_ratings - u2_mean) ** 2).sum())\n",
    "\n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "\n",
    "    result = numerator / denominator\n",
    "\n",
    "    if not np.isfinite(result):\n",
    "        return 0.0\n",
    "\n",
    "    return result\n",
    "\n",
    "# compute similarity matrix of size len(users), len(users).\n",
    "def compute_user_similarity_matrix(train_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    users = train_data['user_id'].unique()\n",
    "    user_similarity_matrix = pd.DataFrame(np.zeros((len(users), len(users))), index=users, columns=users)\n",
    "\n",
    "    for i, usera in enumerate(users):\n",
    "        for j, userb in enumerate(users):\n",
    "            if i > j:\n",
    "                continue\n",
    "            if usera == userb:\n",
    "                user_similarity_matrix.loc[usera, userb] = 1\n",
    "            else:\n",
    "                sim = pearson_correlation(train_data[train_data[\"user_id\"] == usera].set_index(\"item_id\")[\"rating\"], train_data[train_data[\"user_id\"] == userb].set_index(\"item_id\")[\"rating\"])\n",
    "                user_similarity_matrix.loc[usera, userb] = sim\n",
    "                user_similarity_matrix.loc[userb, usera] = sim\n",
    "\n",
    "    return user_similarity_matrix\n",
    "\n",
    "# Get top-k most similar users to the target user and return List of tuples: [(neighbor_user_id, similarity), ...] sorted by similarity descending\n",
    "def get_k_user_neighbors(train_data, user_similarity_matrix: pd.DataFrame, target_user, target_item, k=5):\n",
    "    top_k_neighbors = []\n",
    "\n",
    "    # we want to select top k neighbours who have rated the target item\n",
    "    rated_users = train_data.loc[train_data['item_id'] == target_item, 'user_id'].unique()\n",
    "\n",
    "    user_similarities = user_similarity_matrix.loc[target_user].drop(target_user)\n",
    "    # Only keep users who have rated the target item\n",
    "    user_similarities = user_similarities.loc[user_similarities.index.isin(rated_users)]\n",
    "    top_k_neighbors = list(user_similarities.nlargest(k).items())\n",
    "\n",
    "    return top_k_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EC4rj05jW3y_",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:18.388421Z",
     "start_time": "2025-10-24T12:45:18.351608Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1761075985004,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "EC4rj05jW3y_",
    "outputId": "8f6a30c3-9b55-45e0-a624-5efb045e00c5"
   },
   "outputs": [],
   "source": [
    "path_user_similarity_matrix = os.path.join(OUTPUTS_PATH, \"user_similarity_matrix.pkl\")\n",
    "\n",
    "if os.path.exists(path_user_similarity_matrix):\n",
    "    print(f\"Loading user similarity matrix from: {path_user_similarity_matrix} üíæ\")\n",
    "    user_similarity_matrix = pd.read_pickle(path_user_similarity_matrix)\n",
    "    print(\"‚úÖ Loaded successfully!\")\n",
    "else:\n",
    "    print(\"Computing user similarity matrix... ‚è≥\")\n",
    "    user_similarity_matrix = compute_user_similarity_matrix(train_data)\n",
    "    user_similarity_matrix.to_pickle(path_user_similarity_matrix)\n",
    "    print(f\"‚úÖ Computed and saved to shared drive for future use!\")\n",
    "\n",
    "# Verify the index\n",
    "print(f\"User similarity matrix shape: {user_similarity_matrix.shape}\")\n",
    "print(f\"Is user 943 in user_similarity_matrix index after loading/computing: {943 in user_similarity_matrix.index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R0NSep-vsAdL",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:18.420037Z",
     "start_time": "2025-10-24T12:45:18.410267Z"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1761075985006,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "R0NSep-vsAdL"
   },
   "outputs": [],
   "source": [
    "def predict_rating_user_based(\n",
    "    train_data: pd.DataFrame,\n",
    "    user_similarity_matrix: pd.DataFrame,\n",
    "    target_user,\n",
    "    target_item,\n",
    "    global_mean_rating, k=5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict rating for target_user and target_item using mean-centered user-based CF.\n",
    "\n",
    "    Parameters:\n",
    "    - ratings: pd.DataFrame with columns ['user_id', 'item_id', 'rating']\n",
    "    - user_similarity_matrix: pd.DataFrame of user-user similarities\n",
    "    - target_user: user ID\n",
    "    - target_item: item ID\n",
    "    - global_mean_rating: global mean rating to use as fallback\n",
    "    - k: number of neighbors to consider\n",
    "\n",
    "    Returns:\n",
    "    - float: predicted rating, or np.nan if not possible\n",
    "    \"\"\"\n",
    "    result = 0.0\n",
    "\n",
    "    ############# Your code here ############\n",
    "    user_ratings = train_data[train_data[\"user_id\"] == target_user].set_index(\"item_id\")[\"rating\"]\n",
    "    item_ratings = train_data[train_data[\"item_id\"] == target_item].set_index(\"user_id\")[\"rating\"]\n",
    "\n",
    "    user_mean = user_ratings.mean()\n",
    "\n",
    "    # if user has no rating, fall back to global mean rating\n",
    "    if pd.isna(user_mean):\n",
    "        user_mean = global_mean_rating\n",
    "\n",
    "    # gets top k neighbors who rated the target item\n",
    "    top_k_neighbors = get_k_user_neighbors(train_data, user_similarity_matrix, target_user, target_item, k)\n",
    "\n",
    "    # If no neighbors, return user mean\n",
    "    if not top_k_neighbors:\n",
    "        return user_mean\n",
    "\n",
    "    neighbor_ids = [n_id for n_id, similarity in top_k_neighbors]\n",
    "    neighbour_id_to_mean_rating = train_data[train_data[\"user_id\"].isin(neighbor_ids)].groupby('user_id')['rating'].mean()\n",
    "    # # global mean for neighbors with no ratings\n",
    "    # neighbour_id_to_mean_rating = neighbour_id_to_mean_rating.fillna(global_mean_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for neighbour_id, similarity in top_k_neighbors:\n",
    "        if neighbour_id in item_ratings.index:\n",
    "            numerator += similarity * (item_ratings.loc[neighbour_id] - neighbour_id_to_mean_rating[neighbour_id])\n",
    "            denominator += abs(similarity)\n",
    "\n",
    "    if denominator == 0:\n",
    "        return user_mean \n",
    "\n",
    "    result = user_mean + (numerator/denominator)\n",
    "\n",
    "    result = np.clip(result, 1, 5)\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zpFXA2OStU6G",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:18.451398Z",
     "start_time": "2025-10-24T12:45:18.428800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1761075985011,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "zpFXA2OStU6G",
    "outputId": "aa0e5211-15cd-4204-b04c-b070752e31e0"
   },
   "outputs": [],
   "source": [
    "target_user, target_item, k = 1, 17, 50\n",
    "global_mean_rating = train_data[\"rating\"].mean()\n",
    "print(\n",
    "    f\"The actual rating for user {target_user} and item {target_item} is 3. The predicted rating by user-based CF for user {target_user} and item {target_item} is {predict_rating_user_based(train_data, user_similarity_matrix, target_user, target_item, global_mean_rating, k):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5e4d7e",
   "metadata": {},
   "source": [
    "### Function that generates top-k recommendation list for a target user with user-based recommendataion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a9a890",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:23.780931Z",
     "start_time": "2025-10-24T12:45:18.486750Z"
    }
   },
   "outputs": [],
   "source": [
    "def recommend_topk_user_based(train_data, user_similarity_matrix, target_user, k_user, k=5):\n",
    "    \"\"\"\n",
    "    Generate Top-K recommendations for a target user using User-based CF.\n",
    "\n",
    "    Args:\n",
    "        train_data (pd.DataFrame): ratings data with columns [user_id, item_id, rating]\n",
    "        user_similarity_matrix (pd.DataFrame): precomputed user-user similarity matrix\n",
    "        target_user (int): user_id of the target user\n",
    "        k (int): number of items to recommend\n",
    "\n",
    "    Returns:\n",
    "        list of (item_id, predicted_score) sorted by score desc\n",
    "    \"\"\"\n",
    "    result = []\n",
    "\n",
    "    user_rated_items = set(train_data[train_data[\"user_id\"] == target_user][\"item_id\"])\n",
    "    all_items = set(train_data[\"item_id\"].unique())\n",
    "    candidate_items = all_items - user_rated_items\n",
    "\n",
    "    predictions = []\n",
    "    for item_id in candidate_items:\n",
    "        pred_rating = predict_rating_user_based(\n",
    "            train_data,\n",
    "            user_similarity_matrix,\n",
    "            target_user,\n",
    "            item_id,\n",
    "            global_mean_rating,\n",
    "            k_user,\n",
    "        )\n",
    "        if not np.isnan(pred_rating):  # only keep valid predictions\n",
    "            predictions.append((item_id, pred_rating))\n",
    "\n",
    "    result = sorted(predictions, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "target_user, k_user, k = 1, 50, 50\n",
    "recommendations = recommend_topk_user_based(\n",
    "    train_data, user_similarity_matrix, target_user, k_user, k\n",
    ")\n",
    "print(f\"Top-{k} recommendations for user {target_user} with user-based recommendation:\")\n",
    "for item, score in recommendations:\n",
    "    print(f\"Item {item}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QoQRNu8aqSUz",
   "metadata": {
    "id": "QoQRNu8aqSUz"
   },
   "source": [
    "### Computing RMSE of the User based KNN recommendation model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-WfE6nKfqSyw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:23.892263Z",
     "start_time": "2025-10-24T12:45:23.879239Z"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1761075985012,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "-WfE6nKfqSyw"
   },
   "outputs": [],
   "source": [
    "def evaluate_rating_prediction_user_based(train_data, test_data, user_similarity_matrix, k):\n",
    "    actual_ratings, pred_ratings = [], []\n",
    "    global_mean_rating = train_data[\"rating\"].mean()\n",
    "\n",
    "\n",
    "    for index, test_row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Processing test data\"):\n",
    "        user_id = test_row[\"user_id\"]\n",
    "        item_id = test_row[\"item_id\"]\n",
    "\n",
    "        pred_rating = predict_rating_user_based(train_data, user_similarity_matrix, user_id, item_id, global_mean_rating, k)\n",
    "\n",
    "        if not np.isnan(pred_rating):\n",
    "            actual_ratings.append(test_row['rating'])\n",
    "            pred_ratings.append(pred_rating)\n",
    "\n",
    "    # Given predicted ratings, map them into [1,5] interval using -> 1 + (pred - min_val) * (4 / (max_val - min_val))\n",
    "\n",
    "    min_rating = min(pred_ratings)\n",
    "    max_rating = max(pred_ratings)\n",
    "\n",
    "    if max_rating == min_rating:\n",
    "        min_rating = max_rating - 0.001\n",
    "\n",
    "    mapped_ratings = [1 + (rating - min_rating) * 4 / (max_rating - min_rating) for rating in pred_ratings]\n",
    "\n",
    "    mae_value, mse_value, rmse_value = 0.0, 0.0, 0.0\n",
    "\n",
    "    # compute the metrics: MAE, MSE, RMSE\n",
    "    # mae_value = MAE(actual_ratings, mapped_ratings)\n",
    "    # mse_value = MSE(actual_ratings, mapped_ratings)\n",
    "    rmse_value = RMSE(actual_ratings, mapped_ratings)\n",
    "\n",
    "    return rmse_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1R8yyCKEqbl4",
   "metadata": {
    "id": "1R8yyCKEqbl4"
   },
   "source": [
    "### Use the RMSE loss to find out which combination of content representation and aggregation method works the best for content-recommendation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G8zoktfgqeOR",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:10:16.136998Z",
     "start_time": "2025-10-24T13:10:16.128968Z"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1761081484666,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "G8zoktfgqeOR"
   },
   "outputs": [],
   "source": [
    "def hyperparameter_search_user_based(train_data, val_data, user_similarity_matrix, ks):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for user KNN based recommendation. It finds the best k value\n",
    "\n",
    "    Args:\n",
    "        train_data (pd.DataFrame): training set with columns ['user_id', 'item_id', 'rating']\n",
    "        val_data (pd.DataFrame): validation set with columns ['user_id', 'item_id', 'rating']\n",
    "        user_similarity_matrix: user similarity matrix\n",
    "        ks: list of ks to test out\n",
    "\n",
    "    Returns:\n",
    "            dict: {'best_k': int, 'best_rmse': float, 'results': list of tuples (k, rmse)}\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n=== User Based Recommendation Hyperparameter Search ===\")\n",
    "    results = []\n",
    "\n",
    "    for k in ks:\n",
    "          print(f\"Testing k value={k}\")\n",
    "          rmse = evaluate_rating_prediction_user_based(train_data, val_data, user_similarity_matrix, k)\n",
    "\n",
    "          results.append({\n",
    "              'k': k,\n",
    "              'RMSE': rmse,\n",
    "          })\n",
    "          print(f\"Config: k={k} -> RMSE = {rmse:.4f}\")\n",
    "\n",
    "    # Get the best performing combinations as the result of this hyperparameter tuning\n",
    "    best_result = min(results, key=lambda x: x['RMSE'])\n",
    "    best_k = best_result['k']\n",
    "    best_rmse = best_result['RMSE']\n",
    "    print(f\"\\n‚úÖ Best UserKNN config: k={best_k}, RMSE={best_rmse:.4f}\")\n",
    "\n",
    "    return {'best_k': best_k, 'best_rmse': best_rmse, 'results': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lmO-3xPmqepz",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:17:49.364085Z",
     "start_time": "2025-10-24T13:10:19.863673Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1404516,
     "status": "ok",
     "timestamp": 1761082889615,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "lmO-3xPmqepz",
    "outputId": "38adbc23-3df1-4d79-8014-66fec94dde5b"
   },
   "outputs": [],
   "source": [
    "path_user_knn_tuning = os.path.join(OUTPUTS_PATH, \"user_knn_tuning_result.pkl\")\n",
    "ks = [5, 10, 20, 30, 50, 70, 100]\n",
    "\n",
    "user_knn_tuning = None\n",
    "\n",
    "if os.path.exists(path_user_knn_tuning):\n",
    "    print(f\"Loading UserKNN tuning results from: {path_user_knn_tuning} üíæ\")\n",
    "    with open(path_user_knn_tuning, 'rb') as f:\n",
    "        user_knn_tuning = pickle.load(f)\n",
    "    print(\"‚úÖ Loaded successfully!\")\n",
    "else:\n",
    "    print(\"Running UserKNN hyperparameter search... ‚è≥\")\n",
    "    user_knn_tuning = hyperparameter_search_user_based(train_data, val_data, user_similarity_matrix, ks)\n",
    "    with open(path_user_knn_tuning, 'wb') as f:\n",
    "        pickle.dump(user_knn_tuning, f)\n",
    "    print(f\"‚úÖ Saved UserKNN tuning results to {path_user_knn_tuning}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chHfq5rZtSgT",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:17:59.582363Z",
     "start_time": "2025-10-24T13:17:59.444749Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1761083050979,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "chHfq5rZtSgT",
    "outputId": "bdf29031-57ca-483f-84d9-9aa82634a4f7"
   },
   "outputs": [],
   "source": [
    "# Visualize UserKNN hyperparameter tuning results\n",
    "\n",
    "results_df = pd.DataFrame(user_knn_tuning['results'], columns=['k', 'RMSE'])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(results_df['k'], results_df['RMSE'], marker='o', linestyle='-', linewidth=2)\n",
    "plt.title(\"User-based CF Hyperparameter Tuning (RMSE vs. k)\")\n",
    "plt.xlabel(\"Number of Neighbours (k)\")\n",
    "plt.ylabel(\"Validation RMSE\")\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Highlight the best k\n",
    "best_k = user_knn_tuning['best_k']\n",
    "best_rmse = user_knn_tuning['best_rmse']\n",
    "rmse_user_based = best_rmse\n",
    "plt.scatter(best_k, best_rmse, color='red', s=100, label=f\"Best k={best_k} (RMSE={best_rmse:.4f})\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wuOCOGREIQFh",
   "metadata": {
    "id": "wuOCOGREIQFh"
   },
   "source": [
    "## 1.3 Item-based neighborhood method (ItemKNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coDZ3EC8qg7d",
   "metadata": {
    "id": "coDZ3EC8qg7d"
   },
   "source": [
    "The **cosine similarity** between two items $i$ and $j$ is defined as:\n",
    "\n",
    "$$\n",
    "\\text{sim}(i,j) = \\frac{\\sum_{u \\in U_{ij}} r_{u,i} \\cdot r_{u,j}}\n",
    "                      {\\sqrt{\\sum_{u \\in U_{ij}} r_{u,i}^2} \\cdot \\sqrt{\\sum_{u \\in U_{ij}} r_{u,j}^2}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $r_{u,i}$ = rating of user $u$ on item $i$\n",
    "- $r_{u,j}$ = rating of user $u$ on item $j$\n",
    "- $U_{ij}$ = set of users who have rated both items $i$ and $j$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "va6rJ7eeCFJ4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:24.172690Z",
     "start_time": "2025-10-24T12:45:24.159398Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1761083084407,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "va6rJ7eeCFJ4",
    "outputId": "82c28526-4108-4d2e-b715-15abd9e668a1"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(item1_ratings: pd.Series, item2_ratings: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two items' rating vectors.\n",
    "    Only common users are considered.\n",
    "\n",
    "    Parameters:\n",
    "    - item1_ratings, item2_ratings: pd.Series indexed by user_id\n",
    "\n",
    "    Returns:\n",
    "    - float: cosine similarity between -1 and 1\n",
    "    \"\"\"\n",
    "    result = 0.0\n",
    "\n",
    "    common_users = item1_ratings.index.intersection(item2_ratings.index)\n",
    "    if len(common_users) == 0:\n",
    "        return 0.0  # no common users, similarity is 0\n",
    "\n",
    "    x = item1_ratings[common_users].values\n",
    "    y = item2_ratings[common_users].values\n",
    "\n",
    "    numerator = np.dot(x, y)\n",
    "    denominator = np.sqrt(np.dot(x, x)) * np.sqrt(np.dot(y, y))\n",
    "    if denominator == 0:\n",
    "        return 0.0  # avoid division by zero\n",
    "\n",
    "    result = numerator / denominator\n",
    "\n",
    "    return result\n",
    "\n",
    "item1, item2 = 1, 2\n",
    "item1_ratings = train_data[train_data['item_id'] == item1].set_index('user_id')['rating']\n",
    "item2_ratings = train_data[train_data['item_id'] == item2].set_index('user_id')['rating']\n",
    "print(f\"Cosine similarity between items {item1} and {item2} is {cosine_similarity(item1_ratings, item2_ratings):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0BmkPrqrfx",
   "metadata": {
    "id": "9a0BmkPrqrfx"
   },
   "source": [
    "### Item-Item Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hmpmwEt5CN44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:24.234696Z",
     "start_time": "2025-10-24T12:45:24.215784Z"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1761083086642,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "hmpmwEt5CN44"
   },
   "outputs": [],
   "source": [
    "def compute_item_similarity_matrix(train_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute item-item similarity matrix using cosine similarity.\n",
    "\n",
    "    Parameters:\n",
    "    - ratings: pd.DataFrame with columns ['user_id', 'item_id', 'rating']\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: item-item similarity matrix (rows & cols = item_ids)\n",
    "    \"\"\"\n",
    "    items = train_data['item_id'].unique()\n",
    "    item_similarity_matrix = pd.DataFrame(np.zeros((len(items), len(items))), index=items, columns=items)\n",
    "\n",
    "    for i, i1 in enumerate(items):\n",
    "        i1_ratings = train_data[train_data['item_id'] == i1].set_index('user_id')['rating']\n",
    "        for j, i2 in enumerate(items):\n",
    "            if j > i:  # Only compute for the upper triangle\n",
    "                i2_ratings = train_data[train_data['item_id'] == i2].set_index('user_id')['rating']\n",
    "                sim = cosine_similarity(i1_ratings, i2_ratings)\n",
    "                item_similarity_matrix.at[i1, i2] = sim\n",
    "                item_similarity_matrix.at[i2, i1] = sim  # Mirror the value\n",
    "            elif i == j:\n",
    "                item_similarity_matrix.at[i1, i2] = 1.0  # Similarity with self is 1\n",
    "\n",
    "    return item_similarity_matrix\n",
    "\n",
    "# start_time = time.time()\n",
    "# item_similarity_matrix = compute_item_similarity_matrix(train_data)\n",
    "# end_time = time.time()\n",
    "# print(f'Running time: {end_time - start_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JZ4Xhb8NZOvE",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:24.281416Z",
     "start_time": "2025-10-24T12:45:24.241709Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 912,
     "status": "ok",
     "timestamp": 1761083089359,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "JZ4Xhb8NZOvE",
    "outputId": "38506686-af82-4564-9d8d-ceeb8380ba94"
   },
   "outputs": [],
   "source": [
    "path_item_similarity_matrix = os.path.join(OUTPUTS_PATH, \"item_similarity_matrix.pkl\")\n",
    "\n",
    "if os.path.exists(path_item_similarity_matrix):\n",
    "    print(f\"Loading item similarity matrix from: {path_item_similarity_matrix} üíæ\")\n",
    "    item_similarity_matrix = pd.read_pickle(path_item_similarity_matrix)\n",
    "\n",
    "    # ensure IDs are int\n",
    "    item_similarity_matrix.index = item_similarity_matrix.index.astype(int)\n",
    "    item_similarity_matrix.columns = item_similarity_matrix.columns.astype(int)\n",
    "\n",
    "    print(f\"‚úÖ Loaded successfully with shape {item_similarity_matrix.shape} and item IDs preserved!\")\n",
    "else:\n",
    "    print(\"Computing item similarity matrix... ‚è≥\")\n",
    "    start_time = time.time()\n",
    "    item_similarity_matrix = compute_item_similarity_matrix(train_data)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # ensure IDs are int\n",
    "    item_similarity_matrix.index = item_similarity_matrix.index.astype(int)\n",
    "    item_similarity_matrix.columns = item_similarity_matrix.columns.astype(int)\n",
    "\n",
    "    # save properly with labels\n",
    "    item_similarity_matrix.to_pickle(path_item_similarity_matrix)\n",
    "    print(f\"‚úÖ Computed and saved to {path_item_similarity_matrix} (took {end_time - start_time:.2f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XifedD9isb1l",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:24.313118Z",
     "start_time": "2025-10-24T12:45:24.304981Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1761083094394,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "XifedD9isb1l",
    "outputId": "a69aef90-8ce5-4173-a229-72e3326627db"
   },
   "outputs": [],
   "source": [
    "print(\"Matrix shape:\", item_similarity_matrix.shape)\n",
    "print(\"Index dtype:\", item_similarity_matrix.index.dtype)\n",
    "print(\"Column dtype:\", item_similarity_matrix.columns.dtype)\n",
    "print(\"Sample item IDs:\", item_similarity_matrix.index[:10].tolist())\n",
    "print(\"Accessing item 599 row works?\", 599 in item_similarity_matrix.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Lg3eGgG1JASZ",
   "metadata": {
    "id": "Lg3eGgG1JASZ"
   },
   "source": [
    "\n",
    "> Instead of calculating the similarity for all pairs, we only compute it for one half of the matrix (upper or lower triangle) and then mirror the values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-KO4aCyLqzOR",
   "metadata": {
    "id": "-KO4aCyLqzOR"
   },
   "source": [
    "### Get k most similar item along with the similarity values to a target item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5B4S0OplQwZw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:24.375607Z",
     "start_time": "2025-10-24T12:45:24.364895Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1761083095608,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "5B4S0OplQwZw",
    "outputId": "01faf4a8-cc62-43bf-e5a8-95c951f1f2ee"
   },
   "outputs": [],
   "source": [
    "def get_k_item_neighbors(item_similarity_matrix: pd.DataFrame, target_item, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve top-k most similar items to the target item.\n",
    "\n",
    "    Parameters:\n",
    "    - item_similarity_matrix: pd.DataFrame, item-item similarity\n",
    "    - target_item: item ID\n",
    "    - k: number of neighbors\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples: [(neighbor_item_id, similarity), ...]\n",
    "    \"\"\"\n",
    "    top_k_neighbors = []\n",
    "\n",
    "    similarities = item_similarity_matrix.loc[target_item].drop(index=target_item)\n",
    "    top_k_neighbors = similarities.sort_values(ascending=False).head(k)\n",
    "    top_k_neighbors = list(zip(top_k_neighbors.index, top_k_neighbors.values))\n",
    "\n",
    "    return top_k_neighbors\n",
    "\n",
    "target_item, k = 1, 10\n",
    "print(f\"Neighbors of item {target_item} are:\")\n",
    "get_k_item_neighbors(item_similarity_matrix, target_item, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4l19jFyMq8hm",
   "metadata": {
    "id": "4l19jFyMq8hm"
   },
   "source": [
    "### Function that predicts the rating that a target user might give to a target item using item-item similarity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M5NV_FNvq7MG",
   "metadata": {
    "id": "M5NV_FNvq7MG"
   },
   "source": [
    "The **predicted rating** for a target user $u$ on a target item $i$ using item-based collaborative filtering is:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{u,i} = \\frac{\\sum_{j \\in N(i)} s(i,j) \\cdot r_{u,j}}{\\sum_{j \\in N(i)} |s(i,j)|}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\hat{r}_{u,i}$ = predicted rating of user $u$ on item $i$\n",
    "- $N(i)$ = set of top-$k$ most similar items to item $i$ that user $u$ has rated\n",
    "- $s(i,j)$ = similarity between item $i$ and item $j$\n",
    "- $r_{u,j}$ = rating of user $u$ on item $j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lS7GKnS6Q7dU",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:24.451074Z",
     "start_time": "2025-10-24T12:45:24.428029Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1761083096991,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "lS7GKnS6Q7dU",
    "outputId": "dbf70c91-f8c7-40bc-f92a-189e667f1626"
   },
   "outputs": [],
   "source": [
    "def predict_rating_item_based(train_data: pd.DataFrame, item_similarity_matrix: pd.DataFrame, target_user, target_item, k=5):\n",
    "    \"\"\"\n",
    "    Predict rating using item-based CF (non-mean centric).\n",
    "\n",
    "    Parameters:\n",
    "    - ratings: pd.DataFrame ['user_id', 'item_id', 'rating']\n",
    "    - item_similarity_matrix: item-item similarity DataFrame\n",
    "    - target_user: user ID\n",
    "    - target_item: item ID\n",
    "    - k: number of neighbors to use\n",
    "\n",
    "    Returns:\n",
    "    - float: predicted rating, or np.nan if not enough data\n",
    "    \"\"\"\n",
    "    result = 0.0\n",
    "\n",
    "    user_ratings_all = train_data[train_data[\"user_id\"] == target_user][\"rating\"]\n",
    "    user_mean = user_ratings_all.mean()\n",
    "\n",
    "    if target_item not in item_similarity_matrix.index:\n",
    "        return user_mean\n",
    "\n",
    "    user_ratings = train_data[train_data['user_id'] == target_user][['item_id', 'rating']]\n",
    "    rated_items = user_ratings['item_id'].values\n",
    "\n",
    "    # Get top-k similar items to target_item that user has rated\n",
    "    similarities = item_similarity_matrix.loc[target_item, rated_items]\n",
    "    top_k_idx = np.argsort(similarities.values)[::-1][:k]\n",
    "    top_k_items = rated_items[top_k_idx]\n",
    "    top_k_sims = similarities.values[top_k_idx]\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "    for item, sim in zip(top_k_items, top_k_sims):\n",
    "        rating = user_ratings[user_ratings['item_id'] == item]['rating'].values[0]\n",
    "        numerator += sim * rating\n",
    "        denominator += abs(sim)\n",
    "\n",
    "    if denominator == 0:\n",
    "        # If no similar items, return user's mean rating or np.nan\n",
    "        return user_mean\n",
    "\n",
    "    result = numerator / denominator\n",
    "\n",
    "    return result\n",
    "\n",
    "target_user, target_item, k = 1, 17, 50\n",
    "print(f\"The actual rating for user {target_user} and item {target_item} is 3. The predicted rating by item-based CF for user {target_user} and item {target_item} is {predict_rating_item_based(train_data, item_similarity_matrix, target_user, target_item, k):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hh8Q3qrUrF4K",
   "metadata": {
    "id": "hh8Q3qrUrF4K"
   },
   "source": [
    "### Function that generates top-k recommendation list for a target user with item-based recommendataion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uQfuQcbvRmfN",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:40.649529Z",
     "start_time": "2025-10-24T12:45:24.487908Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24044,
     "status": "ok",
     "timestamp": 1761083121916,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "uQfuQcbvRmfN",
    "outputId": "992f4ca5-d448-4a3e-db97-13f99ec35ef3"
   },
   "outputs": [],
   "source": [
    "def recommend_topk_item_based(train_data, item_similarity_matrix, target_user, k_item, k=5):\n",
    "    \"\"\"\n",
    "    Generate Top-K recommendations for a target user using Item-based CF.\n",
    "\n",
    "    Args:\n",
    "        train_data (pd.DataFrame): ratings data with columns [user_id, item_id, rating]\n",
    "        item_similarity_matrix (pd.DataFrame): precomputed item-item similarity matrix\n",
    "        target_user (int): user_id of the target user\n",
    "        k (int): number of items to recommend\n",
    "\n",
    "    Returns:\n",
    "        list of (item_id, predicted_score) sorted by score desc\n",
    "    \"\"\"\n",
    "    result = []\n",
    "\n",
    "    user_rated_items = set(train_data[train_data['user_id'] == target_user]['item_id'])\n",
    "    all_items = set(train_data['item_id'].unique())\n",
    "    candidate_items = all_items - user_rated_items\n",
    "\n",
    "    predictions = []\n",
    "    for item_id in candidate_items:\n",
    "        pred_rating = predict_rating_item_based(\n",
    "            train_data, item_similarity_matrix, target_user, item_id, k_item\n",
    "        )\n",
    "        if not np.isnan(pred_rating):  # only keep valid predictions\n",
    "            predictions.append((item_id, pred_rating))\n",
    "\n",
    "    result = sorted(predictions, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "    return result\n",
    "\n",
    "target_user, k_item, k = 1, 50, 50\n",
    "recommendations = recommend_topk_item_based(train_data, item_similarity_matrix, target_user, k_item, k)\n",
    "print(f\"Top-{k} recommendations for user {target_user}:\")\n",
    "for item, score in recommendations:\n",
    "    print(f\"Item {item}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G4oE2x2QrKJg",
   "metadata": {
    "id": "G4oE2x2QrKJg"
   },
   "source": [
    "### Item based CF Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QO04wAMxsXPO",
   "metadata": {
    "id": "QO04wAMxsXPO"
   },
   "source": [
    "> Since this is a rating task, we will be using RMSE to tune the number of neighbours to take into consideration, namely `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f910b8ace4293",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T12:45:40.770799Z",
     "start_time": "2025-10-24T12:45:40.756530Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_rating_prediction_item_based(train_data, val_data, item_similarity_matrix, k):\n",
    "\n",
    "    def RMSE(actual_rating, pred_rating):\n",
    "        return np.mean((np.array(pred_rating) - np.array(actual_rating))**2)**0.5\n",
    "\n",
    "    preds, actuals = [], []\n",
    "\n",
    "    for _, row in val_data.iterrows():\n",
    "        user_id, item_id, true_rating = int(row['user_id']), int(row['item_id']), row['rating']\n",
    "        pred = predict_rating_item_based(train_data, item_similarity_matrix, user_id, item_id, k=k)\n",
    "\n",
    "        # Skip if prediction not possible (NaN)\n",
    "        if not np.isnan(pred):\n",
    "            preds.append(pred)\n",
    "            actuals.append(true_rating)\n",
    "\n",
    "    rmse = RMSE(actuals, preds)\n",
    "    return rmse\n",
    "\n",
    "def hyperparameter_search_item_based(train_data, val_data, item_similarity_matrix, k_values=[5, 10, 20, 30, 50, 70, 100]):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for Item-based CF using validation RMSE.\n",
    "    Only searches over k (number of neighbors), but can be extended easily.\n",
    "\n",
    "    Args:\n",
    "        train_data (pd.DataFrame): training set with columns ['user_id', 'item_id', 'rating']\n",
    "        val_data (pd.DataFrame): validation set with columns ['user_id', 'item_id', 'rating']\n",
    "        item_similarity_matrix (pd.DataFrame): precomputed item-item similarity matrix\n",
    "        k_values (list): list of k values to test\n",
    "\n",
    "    Returns:\n",
    "        dict: {'best_k': int, 'best_rmse': float, 'results': list of tuples (k, rmse)}\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n=== Item-based CF Hyperparameter Search ===\")\n",
    "    results = []\n",
    "\n",
    "    for k in k_values:\n",
    "        print(f\"\\nEvaluating k = {k}...\")\n",
    "        rmse = evaluate_rating_prediction_item_based(train_data, val_data, item_similarity_matrix, k)\n",
    "\n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'RMSE': rmse,\n",
    "        })\n",
    "\n",
    "        if np.isnan(rmse):\n",
    "            print(f\"k={k}: Evaluation failed (no valid predictions).\")\n",
    "        else:\n",
    "            print(f\"k={k}: Validation RMSE = {rmse:.4f}\")\n",
    "\n",
    "    valid_results = [r for r in results if not np.isnan(r['RMSE'])]\n",
    "\n",
    "    if not valid_results:\n",
    "        print(\"Hyperparameter search failed to produce any valid results.\")\n",
    "        return {'best_k': None, 'best_rmse': np.nan, 'results': results}\n",
    "\n",
    "    best_result = min(valid_results, key=lambda x: x['RMSE'])\n",
    "    best_k = best_result['k']\n",
    "    best_rmse = best_result['RMSE']\n",
    "\n",
    "    print(f\"\\n‚úÖ Best ItemKNN config: k={best_k}, RMSE={best_rmse:.4f}\")\n",
    "\n",
    "    return {'best_k': best_k, 'best_rmse': best_rmse, 'results': results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XfMDg3wGFmpp",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:00:04.963846Z",
     "start_time": "2025-10-24T12:45:40.817141Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1761083122073,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "XfMDg3wGFmpp",
    "outputId": "4c38af49-824b-408d-b81c-5f792c1e2d0e"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path_item_knn_tuning = os.path.join(OUTPUTS_PATH, 'item_knn_tuning_results.pkl')\n",
    "\n",
    "\n",
    "if os.path.exists(path_item_knn_tuning):\n",
    "    print(f\"Loading ItemKNN tuning results from: {path_item_knn_tuning} üíæ\")\n",
    "    with open(path_item_knn_tuning, 'rb') as f:\n",
    "        item_knn_tuning = pickle.load(f)\n",
    "    print(\"‚úÖ Loaded successfully!\")\n",
    "else:\n",
    "    print(\"Running ItemKNN hyperparameter search... ‚è≥\")\n",
    "    item_knn_tuning = hyperparameter_search_item_based(train_data, val_data, item_similarity_matrix)\n",
    "    with open(path_item_knn_tuning, 'wb') as f:\n",
    "        pickle.dump(item_knn_tuning, f)\n",
    "    print(f\"‚úÖ Saved ItemKNN tuning results to {path_item_knn_tuning}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rqsB-Ki8tGeh",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:00:54.246874Z",
     "start_time": "2025-10-24T13:00:54.231638Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1761083132930,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "rqsB-Ki8tGeh"
   },
   "outputs": [],
   "source": [
    "best_k = item_knn_tuning['best_k']\n",
    "# takes 20 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb143653c14e4a6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:00:57.034182Z",
     "start_time": "2025-10-24T13:00:56.879598Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize ItemKNN hyperparameter tuning results\n",
    "\n",
    "results_df = pd.DataFrame(item_knn_tuning['results'], columns=['k', 'RMSE'])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(results_df['k'], results_df['RMSE'], marker='o', linestyle='-', linewidth=2)\n",
    "plt.title(\"Item-based CF Hyperparameter Tuning (RMSE vs. k)\")\n",
    "plt.xlabel(\"Number of Neighbours (k)\")\n",
    "plt.ylabel(\"Validation RMSE\")\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Highlight the best k\n",
    "best_k = item_knn_tuning['best_k']\n",
    "best_rmse = item_knn_tuning['best_rmse']\n",
    "rmse_item_based = best_rmse\n",
    "plt.scatter(best_k, best_rmse, color='red', s=100, label=f\"Best k={best_k} (RMSE={best_rmse:.4f})\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe167a2b94f76f1",
   "metadata": {},
   "source": [
    "![image.png](attachment:fa957900-1d21-4a49-8ea6-e0b8aeb2d891.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d851ca6156e4a0",
   "metadata": {},
   "source": [
    "> Based on the results and as seen in the visualization, we take 70 to be the best number of neighbours for item based CF. We have tried both np.nan and user mean as fallback conditions (denominator being zero or the target item not being in the similarity matrix) but they rarely occurred during the evaluation on val_data, so the returned np.nan (and skipped) or returned user_mean (and included its error) do not make a significant difference to the final averaged RMSE. We can verify the low number of such cases by counting them. However, for the sake of the following analysis, we will consider the average as fallback for both UserKNN and ItemKNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c48e06c6b4c8ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:18:59.855703Z",
     "start_time": "2025-10-24T13:18:22.140661Z"
    }
   },
   "outputs": [],
   "source": [
    "# Verify the hypothesis about the number of fallbacks in UserKNN and ItemKNN\n",
    "\n",
    "print(\"Verifying the number of fallbacks during validation for CF models...\")\n",
    "\n",
    "\n",
    "# --- UserKNN Fallback Check ---\n",
    "def count_userknn_fallbacks(train_data, val_data, user_similarity_matrix, k):\n",
    "    fallbacks = 0\n",
    "    total_predictions = 0\n",
    "    global_mean_rating = train_data[\"rating\"].mean()\n",
    "\n",
    "    # Store user means to avoid recomputing\n",
    "    user_means = train_data.groupby('user_id')['rating'].mean().fillna(global_mean_rating)\n",
    "\n",
    "    for index, test_row in tqdm(val_data.iterrows(), total=len(val_data), desc=\"UserKNN Fallback Check\"):\n",
    "        user_id = test_row[\"user_id\"]\n",
    "        item_id = test_row[\"item_id\"]\n",
    "        total_predictions += 1\n",
    "\n",
    "        # Check for unknown user\n",
    "        if user_id not in user_similarity_matrix.index or pd.isna(user_means.get(user_id)):\n",
    "            fallbacks += 1  # Fallback to global mean\n",
    "            continue\n",
    "\n",
    "        # Check for neighbours\n",
    "        rated_users = train_data.loc[train_data['item_id'] == item_id, 'user_id'].unique()\n",
    "        user_similarities = user_similarity_matrix.loc[user_id].drop(user_id)\n",
    "        user_similarities_for_item = user_similarities.loc[user_similarities.index.isin(rated_users)]\n",
    "        top_k_neighbors = user_similarities_for_item.nlargest(k)\n",
    "\n",
    "        if len(top_k_neighbors) == 0:\n",
    "            fallbacks += 1  # Fallback to user mean\n",
    "            continue\n",
    "\n",
    "        # Check for zero denominator\n",
    "        denominator = top_k_neighbors.abs().sum()\n",
    "        if denominator == 0:\n",
    "            fallbacks += 1  # Fallback to user mean\n",
    "\n",
    "    return fallbacks, total_predictions\n",
    "\n",
    "\n",
    "# --- ItemKNN Fallback Check ---\n",
    "def count_itemknn_fallbacks(train_data, val_data, item_similarity_matrix, k):\n",
    "    fallbacks = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Store user means to avoid recomputing\n",
    "    user_means = train_data.groupby('user_id')['rating'].mean()\n",
    "\n",
    "    for index, test_row in tqdm(val_data.iterrows(), total=len(val_data), desc=\"ItemKNN Fallback Check\"):\n",
    "        user_id = test_row[\"user_id\"]\n",
    "        item_id = test_row[\"item_id\"]\n",
    "        total_predictions += 1\n",
    "\n",
    "        user_mean = user_means.get(user_id, train_data['rating'].mean())\n",
    "\n",
    "        # Check for unknown item or unrated user\n",
    "        if item_id not in item_similarity_matrix.index or pd.isna(user_mean):\n",
    "            fallbacks += 1  # Fallback to user mean or global mean\n",
    "            continue\n",
    "\n",
    "        user_ratings = train_data[train_data['user_id'] == user_id][['item_id', 'rating']]\n",
    "        rated_items = user_ratings['item_id'].values\n",
    "\n",
    "        # Check for rated items\n",
    "        if len(rated_items) == 0:\n",
    "            fallbacks += 1  # Fallback to user mean (or global)\n",
    "            continue\n",
    "\n",
    "        # Get top-k similar items to target_item that user has rated\n",
    "        similarities = item_similarity_matrix.loc[item_id].reindex(rated_items).dropna()\n",
    "        top_k_items_sim = similarities.nlargest(k)\n",
    "\n",
    "        if len(top_k_items_sim) == 0:\n",
    "            fallbacks += 1  # Fallback to user mean\n",
    "            continue\n",
    "\n",
    "        # Check for zero denominator\n",
    "        denominator = top_k_items_sim.abs().sum()\n",
    "        if denominator == 0:\n",
    "            fallbacks += 1  # Fallback to user mean\n",
    "\n",
    "    return fallbacks, total_predictions\n",
    "\n",
    "\n",
    "# Use best k from previous tuning (k_user=50, k_item=70)\n",
    "k_user_best = 50\n",
    "k_item_best = 70\n",
    "total_val_data_size = len(val_data)\n",
    "\n",
    "ub_fallbacks, ub_total = count_userknn_fallbacks(train_data, val_data, user_similarity_matrix, k=k_user_best)\n",
    "ib_fallbacks, ib_total = count_itemknn_fallbacks(train_data, val_data, item_similarity_matrix, k=k_item_best)\n",
    "\n",
    "print(\"\\n--- Fallback Verification Results (Validation Set) ---\")\n",
    "print(f\"Total Validation Pairs: {total_val_data_size}\")\n",
    "print(f\"UserKNN (k={k_user_best}): Fallbacks = {ub_fallbacks} ({ub_fallbacks / ub_total * 100:.2f}%)\")\n",
    "print(f\"ItemKNN (k={k_item_best}): Fallbacks = {ib_fallbacks} ({ib_fallbacks / ib_total * 100:.2f}%)\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Conclusion: The number of fallbacks is indeed low, supporting the hypothesis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c11d1b0747a322",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T15:27:35.539497Z",
     "start_time": "2025-10-24T15:27:35.525788Z"
    }
   },
   "outputs": [],
   "source": [
    "# remember the best value for item k and user k to use later\n",
    "best_k_user = user_knn_tuning['best_k']\n",
    "best_k_item = item_knn_tuning['best_k']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f9d4a43a9c00b7",
   "metadata": {},
   "source": [
    "## 1.4 Matrix Factorization model (MF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uhjkhMeL-7dV",
   "metadata": {
    "id": "uhjkhMeL-7dV"
   },
   "source": [
    "### MF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PGx5t32MIW-Z",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:33:13.711030Z",
     "start_time": "2025-10-24T13:33:13.692414Z"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1761083145042,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "PGx5t32MIW-Z"
   },
   "outputs": [],
   "source": [
    "class MFModel(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_factors=32):\n",
    "        super(MFModel, self).__init__()\n",
    "        # User and item embeddings\n",
    "        self.user_emb = nn.Embedding(num_users, num_factors)\n",
    "        self.item_emb = nn.Embedding(num_items, num_factors)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.normal_(self.user_emb.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_emb.weight, std=0.01)\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        # Dot product between user and item embeddings\n",
    "        u = self.user_emb(user_ids)\n",
    "        i = self.item_emb(item_ids)\n",
    "        pred = (u * i).sum(dim=1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AIUtf9lcIdCZ",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:33:15.447019Z",
     "start_time": "2025-10-24T13:33:15.433435Z"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1761083145347,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "AIUtf9lcIdCZ"
   },
   "outputs": [],
   "source": [
    "class MFTrainer:\n",
    "    def __init__(self, num_users, num_items, num_factors=32, lr=0.01, reg=1e-4, epochs=10, device=device, outputs_path=\"outputs\"):\n",
    "        self.device = device\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.outputs_path = os.path.join(outputs_path, 'MF')\n",
    "        os.makedirs(self.outputs_path, exist_ok=True)\n",
    "        self.model_file = os.path.join(self.outputs_path, \"mf_model.pt\")\n",
    "\n",
    "        self.model = MFModel(num_users, num_items, num_factors).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=reg)\n",
    "        self.epochs = epochs\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def train(self, train_data):\n",
    "        users = torch.LongTensor(train_data['user_id'].values - 1).to(self.device)\n",
    "        items = torch.LongTensor(train_data['item_id'].values - 1).to(self.device)\n",
    "        ratings = torch.FloatTensor(train_data['rating'].values).to(self.device)\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(users, items, ratings)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "        print(\"Training MF...\")\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0\n",
    "            for u, i, r in loader:\n",
    "                pred = self.model(u, i)\n",
    "                loss = self.loss_fn(pred, r)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}, Loss={total_loss/len(loader):.4f}\")\n",
    "\n",
    "        # Save trained model\n",
    "        torch.save(self.model.state_dict(), self.model_file)\n",
    "        self.trained = True\n",
    "        print(f\"MF model saved to {self.model_file}\")\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        self.model.eval()\n",
    "        user = torch.LongTensor([user_id - 1]).to(self.device)\n",
    "        item = torch.LongTensor([item_id - 1]).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            pred = self.model(user, item)\n",
    "        return pred.item()\n",
    "\n",
    "    def evaluate(self, data: pd.DataFrame) -> float:\n",
    "        self.model.eval()\n",
    "\n",
    "        users = torch.LongTensor(data[\"user_id\"].values - 1).to(self.device)\n",
    "        items = torch.LongTensor(data[\"item_id\"].values - 1).to(self.device)\n",
    "        ratings = torch.FloatTensor(data[\"rating\"].values).to(self.device)\n",
    "\n",
    "        rmse = 0.0\n",
    "        with torch.no_grad():\n",
    "            preds = self.model(users, items)\n",
    "            rmse = torch.sqrt(torch.mean((preds - ratings) ** 2)).item()\n",
    "\n",
    "        return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IvVff20q--Y9",
   "metadata": {
    "id": "IvVff20q--Y9"
   },
   "source": [
    "### MF Hyperparam Searching and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5QiPWE_AJ8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:33:18.180881Z",
     "start_time": "2025-10-24T13:33:18.159825Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1761083149395,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "be5QiPWE_AJ8"
   },
   "outputs": [],
   "source": [
    "def hyperparameter_search_MF(train_data, val_data, num_users, num_items, device, OUTPUTS_PATH, load_best_only=False):\n",
    "    \"\"\"\n",
    "    Perform grid search over MF hyperparameters.\n",
    "    If load_best_only=True, directly load the previously best model without retraining.\n",
    "    \"\"\"\n",
    "\n",
    "    mf_dir = os.path.join(OUTPUTS_PATH, \"MF\")\n",
    "    os.makedirs(mf_dir, exist_ok=True)\n",
    "\n",
    "    best_config_file = os.path.join(mf_dir, \"best_mf_config.json\")\n",
    "    best_model_file = os.path.join(mf_dir, \"mf_model.pt\")\n",
    "\n",
    "    # ===============================\n",
    "    # Case 1: Only load best model\n",
    "    # ===============================\n",
    "    if load_best_only:\n",
    "        if not os.path.exists(best_config_file) or not os.path.exists(best_model_file):\n",
    "            raise FileNotFoundError(\"‚ö†Ô∏è No saved best MF model found. Run grid search first.\")\n",
    "\n",
    "        with open(best_config_file, \"r\") as f:\n",
    "            best_result = json.load(f)\n",
    "\n",
    "        print(f\"üîπ Loading best MF model: {best_result}\")\n",
    "\n",
    "        best_num_factors = best_result[\"params\"][0]\n",
    "        best_lr = best_result[\"params\"][1]\n",
    "        best_reg = best_result[\"params\"][2]\n",
    "\n",
    "        mf_trainer = MFTrainer(\n",
    "            num_users, num_items,\n",
    "            num_factors=best_num_factors, lr=best_lr, reg=best_reg,\n",
    "            epochs=10, device=device, outputs_path=OUTPUTS_PATH\n",
    "        )\n",
    "\n",
    "        mf_trainer.model.load_state_dict(torch.load(best_model_file, map_location=str(device)))\n",
    "        print(f\"‚úÖ Loaded pre-trained MF model from {best_model_file}\")\n",
    "\n",
    "        print(\"MF Prediction Example:\", mf_trainer.predict(1, 50))\n",
    "        return mf_trainer, best_result[\"rmse\"]\n",
    "\n",
    "    # ===============================\n",
    "    # Case 2: Full grid search\n",
    "    # ===============================\n",
    "    mf_param_grid = {\n",
    "        \"num_factors\": [16, 32, 64],\n",
    "        \"lr\": [0.005, 0.01],\n",
    "        \"reg\": [1e-4, 1e-3],\n",
    "    }\n",
    "\n",
    "    best_result = {\"rmse\": float(\"inf\"), \"params\": None}\n",
    "\n",
    "    print(\"\\n=== MF Hyperparameter Search ===\")\n",
    "    for num_factors, lr, reg in itertools.product(\n",
    "            mf_param_grid[\"num_factors\"],\n",
    "            mf_param_grid[\"lr\"],\n",
    "            mf_param_grid[\"reg\"]):\n",
    "\n",
    "        print(f\"\\nTraining MF with num_factors={num_factors}, lr={lr}, reg={reg}\")\n",
    "        mf_trainer = MFTrainer(\n",
    "            num_users, num_items,\n",
    "            num_factors=num_factors, lr=lr, reg=reg,\n",
    "            epochs=5, device=device, outputs_path=OUTPUTS_PATH\n",
    "        )\n",
    "\n",
    "        mf_trainer.train(train_data)\n",
    "\n",
    "        # ---- Evaluate on validation set using RMSE ----\n",
    "        rmse = mf_trainer.evaluate(val_data)\n",
    "        print(f\"Validation RMSE = {rmse:.4f}\")\n",
    "\n",
    "        if rmse < best_result[\"rmse\"]:\n",
    "            best_result[\"rmse\"] = rmse\n",
    "            best_result[\"params\"] = (num_factors, lr, reg)\n",
    "\n",
    "    print(f\"\\n‚úÖ Best MF Config: factors={best_result['params'][0]}, \"\n",
    "          f\"lr={best_result['params'][1]}, reg={best_result['params'][2]}, \"\n",
    "          f\"RMSE={best_result['rmse']:.4f}\")\n",
    "\n",
    "    # Save best configuration to JSON file\n",
    "    with open(best_config_file, \"w\") as f:\n",
    "        json.dump(best_result, f, indent=4)\n",
    "        print(f\"üíæ Saved best MF configuration to {best_config_file}\")\n",
    "\n",
    "    # ===============================\n",
    "    # Train final best model\n",
    "    # ===============================\n",
    "    best_num_factors, best_lr, best_reg = best_result[\"params\"]\n",
    "    final_mf = MFTrainer(\n",
    "        num_users, num_items,\n",
    "        num_factors=best_num_factors, lr=best_lr, reg=best_reg,\n",
    "        epochs=10, device=device, outputs_path=OUTPUTS_PATH\n",
    "    )\n",
    "    final_mf.train(train_data)\n",
    "\n",
    "    # Save the best model as a fixed filename\n",
    "    torch.save(final_mf.model.state_dict(), best_model_file)\n",
    "    print(f\"üíæ Best MF model saved to {best_model_file}\")\n",
    "\n",
    "    print(\"MF Prediction Example:\", final_mf.predict(1, 50))\n",
    "    return final_mf, best_result[\"rmse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zmDzFaDt_Bc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:48:17.472949Z",
     "start_time": "2025-10-24T13:48:17.418295Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39504,
     "status": "ok",
     "timestamp": 1761083190660,
     "user": {
      "displayName": "Kanta Tanahashi",
      "userId": "14493519528935864142"
     },
     "user_tz": -120
    },
    "id": "zmDzFaDt_Bc5",
    "outputId": "cfbd74c2-eb4d-49ca-a4d9-1534d2b043e5"
   },
   "outputs": [],
   "source": [
    "# mf_model, rmse_mf = hyperparameter_search_MF(train_data, val_data, num_users, num_items, device, OUTPUTS_PATH)\n",
    "mf_model, rmse_mf = hyperparameter_search_MF(train_data, val_data, num_users, num_items, device, OUTPUTS_PATH, load_best_only=True)  # debug only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e218b326",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:47:33.057904Z",
     "start_time": "2025-10-24T13:47:33.044742Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Minimum Validation RMSE from MF: {rmse_mf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe1319a",
   "metadata": {},
   "source": [
    "### Recommend Top K items based on matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4804d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:47:37.354464Z",
     "start_time": "2025-10-24T13:47:37.345233Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def recommend_topk_mf(\n",
    "    mf_trainer: MFTrainer, train_data: pd.DataFrame, target_user: int, k: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates Top-K recommendations for a target user using the trained MF model.\n",
    "\n",
    "    This function predicts scores for all items the user has not yet rated\n",
    "    in a single batch for efficiency.\n",
    "\n",
    "    Args:\n",
    "        mf_trainer (MFTrainer): The trained MFTrainer object from your code.\n",
    "        train_data (pd.DataFrame): Training data used to find already-rated items.\n",
    "        target_user (int): The target user ID (assumed to be 1-indexed).\n",
    "        k (int): The number of items to recommend.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of k item IDs (1-indexed), sorted by predicted score.\n",
    "    \"\"\"\n",
    "    mf_trainer.model.eval()\n",
    "    user_rated_items = set(train_data[train_data[\"user_id\"] == target_user][\"item_id\"])\n",
    "    all_items = set(range(1, mf_trainer.num_items + 1))\n",
    "    candidate_items = all_items - user_rated_items\n",
    "    candidate_items_list = sorted(list(candidate_items))\n",
    "\n",
    "    # 3. Prepare tensors for batch prediction\n",
    "    device = mf_trainer.device\n",
    "\n",
    "    # Create a tensor for the user ID (0-indexed), repeated for all candidate items\n",
    "    users_tensor = (\n",
    "        torch.LongTensor([target_user - 1]).expand(len(candidate_items_list)).to(device)\n",
    "    )\n",
    "\n",
    "    # Create a tensor for all 0-indexed candidate item IDs\n",
    "    items_tensor = torch.LongTensor(np.array(candidate_items_list) - 1).to(device)\n",
    "\n",
    "    # 4. Get all predictions in one batch\n",
    "    with torch.no_grad():\n",
    "        predictions = mf_trainer.model(users_tensor, items_tensor)\n",
    "\n",
    "\n",
    "    scores = predictions.cpu().numpy()\n",
    "    item_scores = list(zip(candidate_items_list, scores))\n",
    "\n",
    "    item_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    top_k_recommendations = [item_id for item_id, score in item_scores[:k]]\n",
    "\n",
    "    return top_k_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ce82c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:47:40.718112Z",
     "start_time": "2025-10-24T13:47:40.658074Z"
    }
   },
   "outputs": [],
   "source": [
    "target_user = 1\n",
    "k = 10\n",
    "\n",
    "recommendations = recommend_topk_mf(\n",
    "    mf_trainer=mf_model, train_data=train_data, target_user=target_user, k=k\n",
    ")\n",
    "\n",
    "print(f\"Top-{k} recommendations based on matrix factorizationfor user {target_user}:\")\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JO8JJvBMIg5R",
   "metadata": {
    "id": "JO8JJvBMIg5R"
   },
   "source": [
    "## 1.5 Bayesian Personalized Ranking method (BPR)\n",
    "\n",
    "BPR is a pairwise ranking optimization method for personalized recommendation. The main idea of BPR is to optimize the ranking of items for each user by maximizing the difference in predicted scores between items that a user has interacted with (positive items) and items that they have not interacted with (negative items).\n",
    "\n",
    "Here, MF is served as the underlying model for BPR. Objective function of BPR (MF-based) is defined as follows:\n",
    "\n",
    "$$\\sum_{(u, i, j) \\in D_S} \\ln \\sigma(\\hat{x}_{ui} - \\hat{x}_{uj}) - \\lambda_{\\Theta} ||\\Theta||^2$$\n",
    "\n",
    "where $D_S$ is the set of all user-item pairs such that user $u$ has interacted with item $i$ (positive item) but not with item $j$ (negative item), $\\sigma$ is the sigmoid function, $\\hat{x}_{ui}$ and $\\hat{x}_{uj}$ are the predicted scores for user $u$ on items $i$ and $j$, respectively, $\\Theta$ represents the model parameters (user and item latent factors), and $\\lambda_{\\Theta}$ is the regularization parameter to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Du-gjYQd_LC-",
   "metadata": {
    "id": "Du-gjYQd_LC-"
   },
   "source": [
    "### BPR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BI7lDbQVIiyi",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:47:43.257678Z",
     "start_time": "2025-10-24T13:47:43.243600Z"
    },
    "id": "BI7lDbQVIiyi"
   },
   "outputs": [],
   "source": [
    "class BPRModel(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_factors=32):\n",
    "        super(BPRModel, self).__init__()\n",
    "        # User and item embeddings without bias\n",
    "        self.user_emb = nn.Embedding(num_users, num_factors)\n",
    "        self.item_emb = nn.Embedding(num_items, num_factors)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.normal_(self.user_emb.weight, std=0.1)\n",
    "        nn.init.normal_(self.item_emb.weight, std=0.1)\n",
    "\n",
    "    def forward(self, user, item_i, item_j):\n",
    "        # Compute difference between positive and negative item scores\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item_i)\n",
    "        j = self.item_emb(item_j)\n",
    "        x_ui = (u * i).sum(dim=1)\n",
    "        x_uj = (u * j).sum(dim=1)\n",
    "        return x_ui - x_uj\n",
    "\n",
    "    def predict(self, user, item):\n",
    "        # Compute preference score for a single item\n",
    "        u = self.user_emb(user)\n",
    "        i = self.item_emb(item)\n",
    "        return (u * i).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PFL1yiKOIk6N",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:48:39.049320Z",
     "start_time": "2025-10-24T13:48:39.034799Z"
    },
    "id": "PFL1yiKOIk6N"
   },
   "outputs": [],
   "source": [
    "class BPRTrainer:\n",
    "    def __init__(self, num_users, num_items, num_factors=32, lr=0.01, reg=1e-4, epochs=10,\n",
    "                 device=device, outputs_path=\"outputs\", batch_size=1024):\n",
    "        self.device = device\n",
    "        self.outputs_path = os.path.join(outputs_path, 'BPR')\n",
    "        os.makedirs(self.outputs_path, exist_ok=True)\n",
    "        self.model_file = os.path.join(self.outputs_path, \"bpr_model.pt\")\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.model = BPRModel(num_users, num_items, num_factors).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=reg)\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def _build_user_pos(self, train_data):\n",
    "        user_pos = defaultdict(set)\n",
    "        for _, row in train_data.iterrows():\n",
    "            user_pos[int(row['user_id']) - 1].add(int(row['item_id']) - 1)\n",
    "        return user_pos\n",
    "\n",
    "    def train(self, train_data):\n",
    "        user_pos = self._build_user_pos(train_data)\n",
    "        num_items = self.model.item_emb.num_embeddings\n",
    "        all_items = np.arange(num_items)\n",
    "\n",
    "        print(\"Training BPR with mini-batch...\")\n",
    "        for epoch in range(self.epochs):\n",
    "            batch_u, batch_i, batch_j = [], [], []\n",
    "            total_loss = 0\n",
    "            for u in user_pos:\n",
    "                pos_items = list(user_pos[u])\n",
    "                if len(pos_items) == 0:\n",
    "                    continue\n",
    "                for i in pos_items:\n",
    "                    # sample negative\n",
    "                    j = np.random.choice(np.setdiff1d(all_items, pos_items))\n",
    "                    batch_u.append(u)\n",
    "                    batch_i.append(i)\n",
    "                    batch_j.append(j)\n",
    "                    # train in batch\n",
    "                    if len(batch_u) >= self.batch_size:\n",
    "                        loss = self._train_batch(batch_u, batch_i, batch_j)\n",
    "                        total_loss += loss\n",
    "                        batch_u, batch_i, batch_j = [], [], []\n",
    "            # train remaining\n",
    "            if batch_u:\n",
    "                loss = self._train_batch(batch_u, batch_i, batch_j)\n",
    "                total_loss += loss\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}, Avg Loss={total_loss/len(user_pos):.4f}\")\n",
    "\n",
    "        torch.save(self.model.state_dict(), self.model_file)\n",
    "        self.trained = True\n",
    "        print(f\"BPR model saved to {self.model_file}\")\n",
    "\n",
    "    def _train_batch(self, batch_u, batch_i, batch_j):\n",
    "        batch_u = torch.LongTensor(batch_u).to(self.device)\n",
    "        batch_i = torch.LongTensor(batch_i).to(self.device)\n",
    "        batch_j = torch.LongTensor(batch_j).to(self.device)\n",
    "\n",
    "        x_uij = self.model(batch_u, batch_i, batch_j)\n",
    "        loss = -torch.log(torch.sigmoid(x_uij)).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def recommend(self, user_id, train_data=None, top_k=10):\n",
    "        self.model.eval()\n",
    "        u = torch.LongTensor([user_id - 1]).to(self.device)\n",
    "        scores = self.model.predict(u, torch.arange(self.model.item_emb.num_embeddings).to(self.device)).cpu().detach().numpy()\n",
    "        if train_data is not None:\n",
    "            interacted = set(train_data[train_data['user_id'] == user_id]['item_id'])\n",
    "        else:\n",
    "            interacted = set()\n",
    "        candidates = [(i+1, s) for i, s in enumerate(scores) if (i+1) not in interacted]\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        return candidates[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m_bsBQD7_M18",
   "metadata": {
    "id": "m_bsBQD7_M18"
   },
   "source": [
    "### BPR Hyperparam Searching and Training\n",
    "\n",
    "> Since BPR is a ranking task, nDCG/MRR/Precision/Recall can be used as target. However, `loss` is used here for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IiUQcl_O_X4_",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T13:48:41.999245Z",
     "start_time": "2025-10-24T13:48:41.980157Z"
    },
    "id": "IiUQcl_O_X4_"
   },
   "outputs": [],
   "source": [
    "def hyperparameter_search_BPR(train_data, num_users, num_items, device, OUTPUTS_PATH, load_best_only=False):\n",
    "    \"\"\"\n",
    "    Perform grid search over BPR hyperparameters.\n",
    "    If load_best_only=True, directly load the previously best model without retraining.\n",
    "    \"\"\"\n",
    "\n",
    "    import json\n",
    "\n",
    "    bpr_dir = os.path.join(OUTPUTS_PATH, \"BPR\")\n",
    "    os.makedirs(bpr_dir, exist_ok=True)\n",
    "\n",
    "    best_config_file = os.path.join(bpr_dir, \"best_bpr_config.json\")\n",
    "    best_model_file = os.path.join(bpr_dir, \"bpr_model.pt\")\n",
    "\n",
    "    # ===============================\n",
    "    # Case 1: Only load best model\n",
    "    # ===============================\n",
    "    if load_best_only:\n",
    "        if not os.path.exists(best_config_file) or not os.path.exists(best_model_file):\n",
    "            raise FileNotFoundError(\"‚ö†Ô∏è No saved best BPR model found. Run grid search first.\")\n",
    "\n",
    "        with open(best_config_file, \"r\") as f:\n",
    "            best_result = json.load(f)\n",
    "\n",
    "        print(f\"üîπ Loading best BPR model: {best_result}\")\n",
    "\n",
    "        best_num_factors = best_result[\"params\"][0]\n",
    "        best_lr = best_result[\"params\"][1]\n",
    "        best_reg = best_result[\"params\"][2]\n",
    "\n",
    "        bpr_trainer = BPRTrainer(\n",
    "            num_users, num_items,\n",
    "            num_factors=best_num_factors, lr=best_lr, reg=best_reg,\n",
    "            epochs=10, device=device, outputs_path=OUTPUTS_PATH\n",
    "        )\n",
    "\n",
    "        bpr_trainer.model.load_state_dict(torch.load(best_model_file, map_location=str(device)))\n",
    "        print(f\"‚úÖ Loaded pre-trained BPR model from {best_model_file}\")\n",
    "\n",
    "        print(\"BPR Top-5 Recommendations:\", bpr_trainer.recommend(1, train_data=train_data, top_k=5))\n",
    "        return bpr_trainer\n",
    "\n",
    "    # ===============================\n",
    "    # Case 2: Full grid search\n",
    "    # ===============================\n",
    "    bpr_param_grid = {\n",
    "        \"num_factors\": [16, 32, 64],\n",
    "        \"lr\": [0.005, 0.01],\n",
    "        \"reg\": [1e-4, 1e-3],\n",
    "    }\n",
    "\n",
    "    best_result = {\"loss\": float(\"inf\"), \"params\": None}\n",
    "\n",
    "    print(\"\\n=== BPR Hyperparameter Search ===\")\n",
    "    for num_factors, lr, reg in itertools.product(\n",
    "            bpr_param_grid[\"num_factors\"],\n",
    "            bpr_param_grid[\"lr\"],\n",
    "            bpr_param_grid[\"reg\"]):\n",
    "\n",
    "        print(f\"\\nTraining BPR with num_factors={num_factors}, lr={lr}, reg={reg}\")\n",
    "        bpr_trainer = BPRTrainer(\n",
    "            num_users, num_items,\n",
    "            num_factors=num_factors, lr=lr, reg=reg,\n",
    "            epochs=5, device=device, outputs_path=OUTPUTS_PATH\n",
    "        )\n",
    "        bpr_trainer.train(train_data)\n",
    "\n",
    "        # ---- Compute average training loss as proxy metric ----\n",
    "        user_pos = bpr_trainer._build_user_pos(train_data)\n",
    "        total_loss = 0\n",
    "        num_samples = 0\n",
    "\n",
    "        for u in list(user_pos.keys())[:200]:  # Sample subset for speed\n",
    "            pos_items = list(user_pos[u])\n",
    "            if len(pos_items) == 0:\n",
    "                continue\n",
    "            j = np.random.randint(0, num_items)\n",
    "            u_tensor = torch.LongTensor([u]).to(device)\n",
    "            i_tensor = torch.LongTensor([pos_items[0]]).to(device)\n",
    "            j_tensor = torch.LongTensor([j]).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                x_uij = bpr_trainer.model(u_tensor, i_tensor, j_tensor)\n",
    "                loss = -torch.log(torch.sigmoid(x_uij)).mean().item()\n",
    "\n",
    "            total_loss += loss\n",
    "            num_samples += 1\n",
    "\n",
    "        avg_loss = total_loss / num_samples\n",
    "        print(f\"Validation Avg Loss = {avg_loss:.4f}\")\n",
    "\n",
    "        if avg_loss < best_result[\"loss\"]:\n",
    "            best_result[\"loss\"] = avg_loss\n",
    "            best_result[\"params\"] = (num_factors, lr, reg)\n",
    "\n",
    "    print(f\"\\n‚úÖ Best BPR Config: factors={best_result['params'][0]}, \"\n",
    "          f\"lr={best_result['params'][1]}, reg={best_result['params'][2]}, \"\n",
    "          f\"Loss={best_result['loss']:.4f}\")\n",
    "\n",
    "    # Save best configuration to JSON file\n",
    "    with open(best_config_file, \"w\") as f:\n",
    "        json.dump(best_result, f, indent=4)\n",
    "        print(f\"üíæ Saved best BPR configuration to {best_config_file}\")\n",
    "\n",
    "    # ===============================\n",
    "    # Train final best model\n",
    "    # ===============================\n",
    "    best_num_factors, best_lr, best_reg = best_result[\"params\"]\n",
    "    final_bpr = BPRTrainer(\n",
    "        num_users, num_items,\n",
    "        num_factors=best_num_factors, lr=best_lr, reg=best_reg,\n",
    "        epochs=10, device=device, outputs_path=OUTPUTS_PATH\n",
    "    )\n",
    "    final_bpr.train(train_data)\n",
    "\n",
    "    # Save best model as fixed filename\n",
    "    torch.save(final_bpr.model.state_dict(), best_model_file)\n",
    "    print(f\"üíæ Best BPR model saved to {best_model_file}\")\n",
    "\n",
    "    print(\"BPR Top-5 Recommendations:\", final_bpr.recommend(1, train_data=train_data, top_k=5))\n",
    "    return final_bpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZbqFr_6M_ZFm",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T14:36:33.576586Z",
     "start_time": "2025-10-24T14:36:33.516126Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 196317,
     "status": "ok",
     "timestamp": 1761057360026,
     "user": {
      "displayName": "Elena Mihalache",
      "userId": "05431839471568078681"
     },
     "user_tz": -120
    },
    "id": "ZbqFr_6M_ZFm",
    "outputId": "9eafd1f2-3a9a-4c1f-884e-b701985e48d5"
   },
   "outputs": [],
   "source": [
    "# bpr_model = hyperparameter_search_BPR(train_data, num_users, num_items, device, OUTPUTS_PATH)\n",
    "bpr_model = hyperparameter_search_BPR(train_data, num_users, num_items, device, OUTPUTS_PATH, load_best_only=True)  # debug only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4439cc05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T14:36:36.060488Z",
     "start_time": "2025-10-24T14:36:36.045432Z"
    }
   },
   "outputs": [],
   "source": [
    "target_user = 1\n",
    "k = 10\n",
    "\n",
    "recommendations = bpr_model.recommend(\n",
    "    target_user, train_data, k\n",
    ")\n",
    "\n",
    "print(f\"Top-{k} recommendations based on Bayesian Personalized Ranking for user {target_user}:\")\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FCzZb89GWyIW",
   "metadata": {
    "id": "FCzZb89GWyIW"
   },
   "source": [
    "## 1.6 Hybrid recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2e2b7c",
   "metadata": {},
   "source": [
    "### 1.6.1 Hybrid model for rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SYab93qu7PeW",
   "metadata": {
    "id": "SYab93qu7PeW"
   },
   "source": [
    "#### Generate component predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac84b4dee1f0b505",
   "metadata": {},
   "source": [
    "The hybrid recommender combines the rating predictions from multiple base models ($C_j$) to generate a final rating.\n",
    "Each base model gives a predicted rating $score_{C_j}(u,i)$ for a user $u$ and item $i$:\n",
    "$$\n",
    "score_{C_j}(u,i)\n",
    "$$\n",
    "The hybrid model learns how to **weight these base model predictions** to produce a final score:\n",
    "$$\n",
    "score(u,i) = \\sum_{j=1}^k \\alpha_j \\cdot score_{C_j}(u,i)\n",
    "$$\n",
    "where the $(\\alpha_j)$ coefficients represent the weight learned for each component, showing how much each model contributes to the final hybrid prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171c47706638c11b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T15:28:19.222051Z",
     "start_time": "2025-10-24T15:28:19.196822Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_component_predictions(val_data, train_data,\n",
    "                                   item_similarity_matrix, user_similarity_matrix,\n",
    "                                   mf_model,\n",
    "                                   k_item=best_k_item, k_user=best_k_user,\n",
    "                                   device=device, verbose=False):\n",
    "    \"\"\"\n",
    "    Generate predictions from multiple recommender components for each (user, item) pair.\n",
    "\n",
    "    Handles both 0-indexed and 1-indexed matrices automatically.\n",
    "    Returns DataFrame with columns:\n",
    "    [user_id, item_id, true_rating, cb, userknn, itemknn, mf]\n",
    "    \"\"\"\n",
    "\n",
    "    preds = []\n",
    "    skipped = 0\n",
    "\n",
    "    # Detect index offset automatically (0-indexed or 1-indexed)\n",
    "    user_min = user_similarity_matrix.index.min()\n",
    "    item_min = item_similarity_matrix.index.min()\n",
    "\n",
    "    user_offset = 1 if user_min == 0 else 0\n",
    "    item_offset = 1 if item_min == 0 else 0\n",
    "\n",
    "    global_mean_rating = train_data[\"rating\"].mean()\n",
    "\n",
    "    if pd.isna(global_mean_rating):\n",
    "        print(\"!!! CRITICAL ERROR: global_mean_rating is NaN !!!\")\n",
    "        print(\"This means your train_data['rating'] column might be all NaNs.\")\n",
    "        print(\"Dropping NaNs from training data for mean calculation...\")\n",
    "        # Try a fallback mean calculation\n",
    "        global_mean_rating = train_data[\"rating\"].dropna().mean()\n",
    "        if pd.isna(global_mean_rating):\n",
    "            raise ValueError(\n",
    "                \"CRITICAL: Still cannot calculate global_mean_rating. Check train_data.\"\n",
    "            )\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Detected user matrix offset: {user_offset}, item matrix offset: {item_offset}\")\n",
    "\n",
    "    for idx, row in val_data.iterrows():\n",
    "        user_id, item_id, true_rating = int(row['user_id']), int(row['item_id']), row['rating']\n",
    "\n",
    "        try:\n",
    "            # --- Content-based prediction (using full embedding, average aggregation) ---\n",
    "            cb_pred = get_user_item_prediction_content_based(train_data, user_id, item_id, 'full', 'weighted_avg')\n",
    "\n",
    "            # --- User-based CF prediction ---\n",
    "            adj_user_id = user_id - user_offset\n",
    "            if adj_user_id not in user_similarity_matrix.index:\n",
    "                raise KeyError(f\"User {adj_user_id} not in similarity matrix\")\n",
    "            ub_pred = predict_rating_user_based(\n",
    "                train_data,\n",
    "                user_similarity_matrix,\n",
    "                user_id,\n",
    "                item_id,\n",
    "                global_mean_rating, k=k_user,\n",
    "            )\n",
    "\n",
    "            # --- Item-based CF prediction ---\n",
    "            adj_item_id = item_id - item_offset\n",
    "            if adj_item_id not in item_similarity_matrix.index:\n",
    "                raise KeyError(f\"Item {adj_item_id} not in similarity matrix\")\n",
    "            ib_pred = predict_rating_item_based(train_data, item_similarity_matrix, user_id, item_id, k=k_item)\n",
    "\n",
    "            # --- Matrix Factorization prediction ---\n",
    "            mf_pred = mf_model.predict(user_id, item_id)\n",
    "\n",
    "            # --- BPR prediction ---\n",
    "            # with torch.no_grad():\n",
    "            #     bpr_model.model.to(device)\n",
    "            #     bpr_pred = bpr_model.model.predict(\n",
    "            #         torch.LongTensor([user_id - 1]).to(device),\n",
    "            #         torch.LongTensor([item_id - 1]).to(device)\n",
    "            #     ).item()\n",
    "\n",
    "            preds.append([user_id, item_id, true_rating, cb_pred, ub_pred, ib_pred, mf_pred])\n",
    "\n",
    "        except Exception as e:\n",
    "            skipped += 1\n",
    "            if verbose:\n",
    "                print(f\"‚ö†Ô∏è Skipped user {user_id}, item {item_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    df_preds = pd.DataFrame(preds, columns=['user_id', 'item_id', 'true_rating', 'cb', 'userknn', 'itemknn', 'mf'])\n",
    "    print(f\"‚úÖ Completed hybrid prediction data generation. Skipped {skipped} problematic pairs.\")\n",
    "    print(f\"Returned shape: {df_preds.shape}\")\n",
    "    return df_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e415664ade0177",
   "metadata": {},
   "source": [
    "#### Train the hybrid regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8777c05ff61f6884",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T15:42:48.324905Z",
     "start_time": "2025-10-24T15:32:22.146144Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate predictions for validation set\n",
    "hybrid_train = generate_component_predictions(\n",
    "    val_data, train_data,\n",
    "    item_similarity_matrix, user_similarity_matrix,\n",
    "    mf_model,\n",
    "    k_item=best_k_item, k_user=best_k_user,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "X = hybrid_train[['cb','userknn','itemknn','mf']]\n",
    "y = hybrid_train['true_rating']\n",
    "\n",
    "# Train regression model\n",
    "hybrid_reg = LinearRegression(fit_intercept=True)\n",
    "\n",
    "hybrid_reg.fit(X, y)\n",
    "\n",
    "# Display learned coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    'Component': X.columns,\n",
    "    'Alpha (Weight)': hybrid_reg.coef_\n",
    "})\n",
    "display(coefficients)\n",
    "print(f\"Intercept (bias): {hybrid_reg.intercept_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ee697af69d523f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T15:09:12.301566Z",
     "start_time": "2025-10-24T15:09:12.276978Z"
    }
   },
   "outputs": [],
   "source": [
    "print(hybrid_train.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9fa18e8590e0c",
   "metadata": {},
   "source": [
    "#### Check ranges and summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b09fffdf81a79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T15:43:55.414438Z",
     "start_time": "2025-10-24T15:43:55.367863Z"
    }
   },
   "outputs": [],
   "source": [
    "# Look at the min, max, mean, std for each component\n",
    "hybrid_train[['cb','userknn','itemknn','mf']].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdf52cf0a20b1e5",
   "metadata": {},
   "source": [
    "#### Train the regression after scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d961cf2a821514fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T15:44:43.833137Z",
     "start_time": "2025-10-24T15:44:43.799934Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select only numeric prediction columns\n",
    "X = hybrid_train[['cb','userknn','itemknn','mf']]\n",
    "\n",
    "# Fit the scaler on training data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train regression on scaled features\n",
    "hybrid_reg_scaled = LinearRegression()\n",
    "hybrid_reg_scaled.fit(X_scaled, hybrid_train['true_rating'])\n",
    "\n",
    "# Show new coefficients\n",
    "coefficients_scaled = pd.DataFrame({\n",
    "    'Component': X.columns,\n",
    "    'Alpha (Weight)': hybrid_reg_scaled.coef_\n",
    "})\n",
    "display(coefficients_scaled)\n",
    "print(f\"Intercept (bias): {hybrid_reg_scaled.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeddb0182875a53",
   "metadata": {},
   "source": [
    "Before scaling the predictors have different magnitudes. The regression absorbs those differences into both coefficients and intercept so the small negative intercept just compensates for overly high CB predictions. After scaling, all predictors are centered (mean=0) and standardized (std=1) so the intercept becomes the mean of the target, i.e. the average movie rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c58cf4fbd1a6f5",
   "metadata": {},
   "source": [
    "#### Make hybrid predictions on test set (maybe move this to Task 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b7a9d967a6435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hybrid_models(\n",
    "    test_data,\n",
    "    train_data,\n",
    "    item_similarity_matrix,\n",
    "    user_similarity_matrix,\n",
    "    mf_model,\n",
    "    hybrid_reg_scaled,\n",
    "    bpr_model,\n",
    "    k_item=best_k_item,\n",
    "    k_user=best_k_user,\n",
    "    device=device,\n",
    "):\n",
    "    # Generate component predictions on test data\n",
    "    hybrid_test = generate_component_predictions(\n",
    "        test_data,\n",
    "        train_data,\n",
    "        item_similarity_matrix,\n",
    "        user_similarity_matrix,\n",
    "        mf_model,\n",
    "        # bpr_model,  # Fixed: no BPR in rating prediction task\n",
    "        k_item=k_item,\n",
    "        k_user=k_user,\n",
    "        device=device,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Predict using learned hybrid regression\n",
    "    X_test = hybrid_test[[\"cb\", \"userknn\", \"itemknn\", \"mf\"]]\n",
    "    y_test = hybrid_test[\"true_rating\"]\n",
    "    y_pred = hybrid_reg.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    test_rmse = RMSE(y_test, y_pred)\n",
    "    print(f\"‚úÖ Hybrid Test RMSE = {test_rmse:.4f}\")\n",
    "\n",
    "    X_test_scaled = scaler.transform(hybrid_test[[\"cb\", \"userknn\", \"itemknn\", \"mf\"]])\n",
    "    y_pred_scaled = hybrid_reg_scaled.predict(X_test_scaled)\n",
    "    test_rmse_scaled = RMSE(hybrid_test[\"true_rating\"], y_pred_scaled)\n",
    "    print(f\"‚úÖ Scaled Hybrid Test RMSE = {test_rmse_scaled:.4f}\")\n",
    "\n",
    "    return test_rmse_scaled\n",
    "\n",
    "rmse_hybrid = evaluate_hybrid_models(\n",
    "    test_data,\n",
    "    train_data,\n",
    "    item_similarity_matrix,\n",
    "    user_similarity_matrix,\n",
    "    mf_model,\n",
    "    hybrid_reg_scaled,\n",
    "    bpr_model,\n",
    "    k_item=best_k_item,\n",
    "    k_user=best_k_user,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c326c4e9b1bf292",
   "metadata": {},
   "source": [
    "> Although scaling did not change RMSE (since linear regression predictions are invariant to affine scaling of inputs), it standardized the component feature space. This gives interpretable weights (comparable across models) and ensures numerical stability. BPR is almost non significant because it is for ranking, not rating prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd1ddf32d9bd3a2",
   "metadata": {},
   "source": [
    "#### Visualize coefficient importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd08aa8d04f0f511",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T15:45:03.632899Z",
     "start_time": "2025-10-24T15:45:03.396288Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "sns.barplot(data=coefficients_scaled, x='Component', y='Alpha (Weight)', palette='coolwarm')\n",
    "plt.title('Hybrid Model Component Importance (After Scaling)')\n",
    "plt.ylabel('Weight (Œ±)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac1532b449a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_compare = pd.DataFrame({\n",
    "    'Component': X.columns,\n",
    "    'Unscaled': hybrid_reg.coef_,\n",
    "    'Scaled': hybrid_reg_scaled.coef_\n",
    "}).melt(id_vars='Component', var_name='Model', value_name='Weight (Œ±)')\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.barplot(data=coeff_compare, x='Component', y='Weight (Œ±)', hue='Model', palette='coolwarm')\n",
    "plt.title('Hybrid Coefficients Before vs After Scaling')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e0bc52",
   "metadata": {},
   "source": [
    "### 1.6.2 Hybrid model for ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d81742",
   "metadata": {},
   "source": [
    "#### Construct pairwise preference dataset\n",
    "\n",
    "For ranking tasks, we optimize using ***pairwise*** preferences (similar to **BPR**). For each user $u$, we create triplets $(u, i, j)$ where:\n",
    "- $i$ is a positively rated item (rating ‚â• 4)\n",
    "- $j$ is a negatively sampled item (not rated or rating < 4)\n",
    "\n",
    "The hybrid ranking model learns to maximize:\n",
    "$$\\mathcal{L} = -\\log \\sigma(f(x_{ui}) - f(x_{uj}))$$\n",
    "\n",
    "where $f(x_{ui})$ is the weighted combination of component predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8545f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairwise_preferences(train_data, user_sample_ratio=0.3, neg_samples_per_pos=1):\n",
    "    \"\"\"\n",
    "    Generate pairwise preference dataset for hybrid ranking model training.\n",
    "    \n",
    "    Args:\n",
    "        train_data (pd.DataFrame): Training ratings data\n",
    "        user_sample_ratio (float): Proportion of users to sample\n",
    "        neg_samples_per_pos (int): Number of negative items per positive item\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Pairwise preferences with columns [user_id, item_i, item_j]\n",
    "                     where item_i is positive, item_j is negative\n",
    "    \"\"\"\n",
    "    pairwise_data = []\n",
    "    \n",
    "    # Sample users for efficiency\n",
    "    all_users = train_data['user_id'].unique()\n",
    "    sampled_users = np.random.choice(all_users, size=int(len(all_users) * user_sample_ratio), replace=False)\n",
    "    \n",
    "    all_items = set(train_data['item_id'].unique())\n",
    "    \n",
    "    for user_id in tqdm(sampled_users, desc=\"Generating pairwise preferences\"):\n",
    "        user_ratings = train_data[train_data['user_id'] == user_id]\n",
    "        \n",
    "        # Positive items: rating >= 4\n",
    "        pos_items = set(user_ratings[user_ratings['rating'] >= 4]['item_id'])\n",
    "        \n",
    "        # Negative candidates: all items - positive items\n",
    "        neg_candidates = list(all_items - pos_items)\n",
    "        \n",
    "        if not pos_items or not neg_candidates:\n",
    "            continue\n",
    "        \n",
    "        # Create pairs\n",
    "        for pos_item in pos_items:\n",
    "            # Sample negative items\n",
    "            num_neg = min(neg_samples_per_pos, len(neg_candidates))\n",
    "            neg_items = np.random.choice(neg_candidates, size=num_neg, replace=False)\n",
    "            \n",
    "            for neg_item in neg_items:\n",
    "                pairwise_data.append([user_id, pos_item, neg_item])\n",
    "    \n",
    "    df_pairs = pd.DataFrame(pairwise_data, columns=['user_id', 'item_i', 'item_j'])\n",
    "    print(f\"‚úÖ Generated {len(df_pairs)} pairwise preferences from {len(sampled_users)} users\")\n",
    "    return df_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f9ffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pairwise preference dataset\n",
    "pairwise_train = generate_pairwise_preferences(train_data, user_sample_ratio=0.3, neg_samples_per_pos=2)\n",
    "print(f\"Pairwise dataset shape: {pairwise_train.shape}\")\n",
    "display(pairwise_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de46a4a6",
   "metadata": {},
   "source": [
    "#### Generate component predictions for pairwise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_component_predictions_pairwise(pairwise_data, train_data,\n",
    "                                            item_similarity_matrix, user_similarity_matrix,\n",
    "                                            mf_model, bpr_model,\n",
    "                                            k_item=20, k_user=70,\n",
    "                                            device='cpu', verbose=False):\n",
    "    \"\"\"\n",
    "    Generate component predictions for pairwise preference data.\n",
    "    \n",
    "    Returns DataFrame with columns:\n",
    "    [user_id, item_i, item_j, cb_i, cb_j, userknn_i, userknn_j, ..., bpr_i, bpr_j]\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    skipped = 0\n",
    "    \n",
    "    global_mean_rating = train_data[\"rating\"].mean()\n",
    "    \n",
    "    for idx, row in tqdm(pairwise_data.iterrows(), total=len(pairwise_data), desc=\"Generating component predictions\"):\n",
    "        user_id = int(row['user_id'])\n",
    "        item_i = int(row['item_i'])\n",
    "        item_j = int(row['item_j'])\n",
    "        \n",
    "        try:\n",
    "            # --- Predictions for positive item i ---\n",
    "            cb_i = get_user_item_prediction_content_based(train_data, user_id, item_i, 'full', 'weighted_avg')\n",
    "            ub_i = predict_rating_user_based(train_data, user_similarity_matrix, user_id, item_i, global_mean_rating, k=k_user)\n",
    "            ib_i = predict_rating_item_based(train_data, item_similarity_matrix, user_id, item_i, k=k_item)\n",
    "            mf_i = mf_model.predict(user_id, item_i)\n",
    "            with torch.no_grad():\n",
    "                bpr_i = bpr_model.model.predict(\n",
    "                    torch.LongTensor([user_id - 1]).to(device),\n",
    "                    torch.LongTensor([item_i - 1]).to(device)\n",
    "                ).item()\n",
    "            \n",
    "            # --- Predictions for negative item j ---\n",
    "            cb_j = get_user_item_prediction_content_based(train_data, user_id, item_j, 'full', 'weighted_avg')\n",
    "            ub_j = predict_rating_user_based(train_data, user_similarity_matrix, user_id, item_j, global_mean_rating, k=k_user)\n",
    "            ib_j = predict_rating_item_based(train_data, item_similarity_matrix, user_id, item_j, k=k_item)\n",
    "            mf_j = mf_model.predict(user_id, item_j)\n",
    "            with torch.no_grad():\n",
    "                bpr_j = bpr_model.model.predict(\n",
    "                    torch.LongTensor([user_id - 1]).to(device),\n",
    "                    torch.LongTensor([item_j - 1]).to(device)\n",
    "                ).item()\n",
    "            \n",
    "            preds.append([user_id, item_i, item_j, \n",
    "                         cb_i, cb_j, ub_i, ub_j, ib_i, ib_j, mf_i, mf_j, bpr_i, bpr_j])\n",
    "            \n",
    "        except Exception as e:\n",
    "            skipped += 1\n",
    "            if verbose:\n",
    "                print(f\"‚ö†Ô∏è Skipped user {user_id}, items ({item_i}, {item_j}): {e}\")\n",
    "            continue\n",
    "    \n",
    "    df_preds = pd.DataFrame(preds, columns=[\n",
    "        'user_id', 'item_i', 'item_j',\n",
    "        'cb_i', 'cb_j', 'userknn_i', 'userknn_j', 'itemknn_i', 'itemknn_j',\n",
    "        'mf_i', 'mf_j', 'bpr_i', 'bpr_j'\n",
    "    ])\n",
    "    print(f\"‚úÖ Completed pairwise prediction generation. Skipped {skipped} problematic pairs.\")\n",
    "    print(f\"Returned shape: {df_preds.shape}\")\n",
    "    return df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34010eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate component predictions for pairwise data\n",
    "hybrid_ranking_train = generate_component_predictions_pairwise(\n",
    "    pairwise_train, train_data,\n",
    "    item_similarity_matrix, user_similarity_matrix,\n",
    "    mf_model, bpr_model,\n",
    "    k_item=20, k_user=70,\n",
    "    device=device,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"Pairwise predictions shape: {hybrid_ranking_train.shape}\")\n",
    "display(hybrid_ranking_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc53705",
   "metadata": {},
   "source": [
    "#### Train hybrid ranking model with BPR loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71293ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRankingModel:\n",
    "    \"\"\"\n",
    "    Hybrid ranking model that learns to combine component predictions using BPR loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_components=5, lr=0.01, reg=1e-4, epochs=20):\n",
    "        self.num_components = num_components\n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # Initialize weights (alphas) randomly\n",
    "        self.weights = np.random.randn(num_components) * 0.1\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def _compute_score(self, features):\n",
    "        \"\"\"Compute hybrid score as weighted sum of components\"\"\"\n",
    "        return np.dot(features, self.weights)\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"Numerically stable sigmoid\"\"\"\n",
    "        return np.where(\n",
    "            x >= 0,\n",
    "            1 / (1 + np.exp(-x)),\n",
    "            np.exp(x) / (1 + np.exp(x))\n",
    "        )\n",
    "    \n",
    "    def fit(self, X_i, X_j):\n",
    "        \"\"\"\n",
    "        Train using pairwise BPR loss.\n",
    "        \n",
    "        Args:\n",
    "            X_i: features for positive items (N, num_components)\n",
    "            X_j: features for negative items (N, num_components)\n",
    "        \"\"\"\n",
    "        # Fit scaler on combined data\n",
    "        all_features = np.vstack([X_i, X_j])\n",
    "        self.scaler.fit(all_features)\n",
    "        \n",
    "        # Scale features\n",
    "        X_i_scaled = self.scaler.transform(X_i)\n",
    "        X_j_scaled = self.scaler.transform(X_j)\n",
    "        \n",
    "        n_samples = len(X_i_scaled)\n",
    "        \n",
    "        print(f\"Training hybrid ranking model for {self.epochs} epochs...\")\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0\n",
    "            \n",
    "            # Compute scores\n",
    "            scores_i = self._compute_score(X_i_scaled)\n",
    "            scores_j = self._compute_score(X_j_scaled)\n",
    "            \n",
    "            # BPR loss: -log(sigmoid(score_i - score_j))\n",
    "            x_uij = scores_i - scores_j\n",
    "            sigmoid_uij = self._sigmoid(x_uij)\n",
    "            \n",
    "            # Compute loss (add epsilon to avoid log(0))\n",
    "            loss = -np.mean(np.log(sigmoid_uij + 1e-10))\n",
    "            total_loss = loss\n",
    "            \n",
    "            # Compute gradient\n",
    "            # d/dw[-log(sigmoid(x_uij))] = -(1 - sigmoid(x_uij)) * (X_i - X_j)\n",
    "            grad_coeff = -(1 - sigmoid_uij).reshape(-1, 1)\n",
    "            grad = np.mean(grad_coeff * (X_i_scaled - X_j_scaled), axis=0)\n",
    "            \n",
    "            # Add L2 regularization gradient\n",
    "            grad += self.reg * self.weights\n",
    "            \n",
    "            # Update weights\n",
    "            self.weights -= self.lr * grad\n",
    "            \n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{self.epochs}, Loss={total_loss:.4f}\")\n",
    "        \n",
    "        print(f\"‚úÖ Training complete. Final loss: {total_loss:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict scores for given features\"\"\"\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self._compute_score(X_scaled)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        \"\"\"Return learned weights\"\"\"\n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d2c568",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "component_names = ['cb', 'userknn', 'itemknn', 'mf', 'bpr']\n",
    "\n",
    "# Features for positive items\n",
    "X_i = hybrid_ranking_train[['cb_i', 'userknn_i', 'itemknn_i', 'mf_i', 'bpr_i']].values\n",
    "\n",
    "# Features for negative items\n",
    "X_j = hybrid_ranking_train[['cb_j', 'userknn_j', 'itemknn_j', 'mf_j', 'bpr_j']].values\n",
    "\n",
    "# Train hybrid ranking model\n",
    "hybrid_rank_model = HybridRankingModel(num_components=5, lr=0.01, reg=1e-4, epochs=100)\n",
    "hybrid_rank_model.fit(X_i, X_j)\n",
    "\n",
    "# Display learned weights\n",
    "learned_weights = hybrid_rank_model.get_weights()\n",
    "hybrid_ranking_weights_df = pd.DataFrame({\n",
    "    'Component': component_names,\n",
    "    'Weight (Œ±)': learned_weights\n",
    "})\n",
    "print(\"\\n--- Learned Component Weights for Ranking ---\")\n",
    "display(hybrid_ranking_weights_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7e94aa",
   "metadata": {},
   "source": [
    "#### Visualize learned ranking weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e60ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=hybrid_ranking_weights_df, x='Component', y='Weight (Œ±)', palette='viridis')\n",
    "plt.title('Hybrid Ranking Model Component Weights (BPR Loss)')\n",
    "plt.ylabel('Weight (Œ±)')\n",
    "plt.axhline(y=0, color='red', linestyle='--', alpha=0.3)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71279a6",
   "metadata": {},
   "source": [
    "#### Generate Top-K recommendations using trained hybrid ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be0a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_topk_hybrid_ranking(user_id, hybrid_rank_model, train_data, item_similarity_matrix, user_similarity_matrix,\n",
    "                                  mf_model, bpr_model, k_item=20, k_user=70, top_k=10, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate Top-K recommendations using the trained hybrid ranking model.\n",
    "    \n",
    "    Args:\n",
    "        user_id (int): Target user.\n",
    "        hybrid_rank_model: Trained HybridRankingModel.\n",
    "        train_data (pd.DataFrame): Training ratings.\n",
    "        item_similarity_matrix (pd.DataFrame): Precomputed item-item similarities.\n",
    "        user_similarity_matrix (pd.DataFrame): Precomputed user-user similarities.\n",
    "        mf_model: Trained MF model.\n",
    "        bpr_model: Trained BPR model.\n",
    "        k_item (int): Number of neighbors for ItemKNN.\n",
    "        k_user (int): Number of neighbors for UserKNN.\n",
    "        top_k (int): Number of items to recommend.\n",
    "        device (str): 'cpu' or 'cuda'.\n",
    "    \n",
    "    Returns:\n",
    "        list of (item_id, hybrid_score)\n",
    "    \"\"\"\n",
    "    global_mean_rating = train_data[\"rating\"].mean()\n",
    "    \n",
    "    # Get items the user already rated\n",
    "    user_rated_items = set(train_data[train_data['user_id'] == user_id]['item_id'])\n",
    "    all_items = set(train_data['item_id'].unique())\n",
    "    candidate_items = all_items - user_rated_items\n",
    "    \n",
    "    predictions = []\n",
    "    for item_id in candidate_items:\n",
    "        try:\n",
    "            # --- Get component predictions ---\n",
    "            cb_pred = get_user_item_prediction_content_based(train_data, user_id, item_id, 'full', 'weighted_avg')\n",
    "            ub_pred = predict_rating_user_based(train_data, user_similarity_matrix, user_id, item_id, global_mean_rating, k=k_user)\n",
    "            ib_pred = predict_rating_item_based(train_data, item_similarity_matrix, user_id, item_id, k=k_item)\n",
    "            mf_pred = mf_model.predict(user_id, item_id)\n",
    "            with torch.no_grad():\n",
    "                bpr_model.model.to(device)\n",
    "                bpr_pred = bpr_model.model.predict(\n",
    "                    torch.LongTensor([user_id - 1]).to(device),\n",
    "                    torch.LongTensor([item_id - 1]).to(device)\n",
    "                ).item()\n",
    "            \n",
    "            # Prepare feature vector for hybrid ranking model\n",
    "            features = np.array([[cb_pred, ub_pred, ib_pred, mf_pred, bpr_pred]])\n",
    "            hybrid_score = hybrid_rank_model.predict(features)[0]\n",
    "            \n",
    "            predictions.append((item_id, hybrid_score))\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Skip if prediction not possible\n",
    "            continue\n",
    "    \n",
    "    # Sort by predicted score descending\n",
    "    recommendations = sorted(predictions, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684bccfbc4fa3048",
   "metadata": {},
   "source": [
    "# Task 2) Experiments for both rating prediction and ranking tasks, and conducting offline evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ad92a3fe0cc156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Precision(ground_truth, rec_list):\n",
    "    # Implement a function that computes Precision across ground truth data and recommendation list generated for each user.\n",
    "    # Note that ground_truth and rec_list contain the list of items for all users, e.g., 2-dimensional arrays.\n",
    "\n",
    "    result = 0.0\n",
    "\n",
    "\n",
    "    ############# Your code here ############\n",
    "    # Precision measures how many of the positive samples are actually positive\n",
    "    # so how many of the recommended items are actually relevant\n",
    "\n",
    "    precisions = []\n",
    "\n",
    "    for gt, rec in zip(ground_truth, rec_list):\n",
    "        if not rec:\n",
    "            continue\n",
    "        hits = len(set(gt) & set(rec))\n",
    "        precision = hits / len(rec)\n",
    "        precisions.append(precision)\n",
    "\n",
    "    result = np.mean(precisions)\n",
    "    #########################################\n",
    "\n",
    "    return result\n",
    "\n",
    "def Recall(ground_truth, rec_list):\n",
    "    # Implement a function that computes Recall across ground truth data and recommendation list generated for each user.\n",
    "    # Note that ground_truth and rec_list contain the list of items for all users, e.g., 2-dimensional arrays.\n",
    "\n",
    "    result = 0.0\n",
    "\n",
    "    ############# Your code here ############\n",
    "    # recall is out of all relevant items, how many did it positively predict\n",
    "    recalls = []\n",
    "\n",
    "    for gt, rec in zip(ground_truth, rec_list):\n",
    "        if not rec or not gt:\n",
    "            continue\n",
    "        hits = len(set(gt) & set(rec))\n",
    "        recall = hits / len(gt)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    result = np.mean(recalls)\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    return result\n",
    "\n",
    "def NDCG(ground_truth, rec_list):\n",
    "    # Implement a function that computes NDCG across ground truth data and recommendation list generated for each user.\n",
    "    # Note that ground_truth and rec_list contain the list of items for all users, e.g., 2-dimensional arrays.\n",
    "\n",
    "    result = 0.0\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ndcgs = []\n",
    "    for gt_items, rec_items in zip(ground_truth, rec_list):\n",
    "        dcg = 0.0\n",
    "\n",
    "        # Calculate DCG\n",
    "        for i, item in enumerate(rec_items):\n",
    "            if item in gt_items:\n",
    "                dcg += 1.0 / np.log2(i + 1)\n",
    "\n",
    "        # Calculate IDCG\n",
    "        idcg = 0.0\n",
    "        for i in range(min(len(rec_items), len(gt_items))):\n",
    "            idcg += 1.0 / np.log2(i + 1)\n",
    "\n",
    "        # Calculate NDCG\n",
    "        if idcg == 0:\n",
    "            ndcgs.append(0.0)\n",
    "        else:\n",
    "            ndcgs.append(dcg / idcg)\n",
    "\n",
    "    return np.mean(ndcgs)\n",
    "    #########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb222b7-1dab-4fad-b9e7-8726e8e9ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Ranking Metrics \"@k\"\n",
    "# -----------------------------\n",
    "def precision_at_k(actual, predicted, k=10):\n",
    "    \"\"\"Precision@K\"\"\"\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "    actual_set = set(actual)\n",
    "    predicted_set = set(predicted)\n",
    "    return len(actual_set & predicted_set) / float(k) if k else 0.0\n",
    "\n",
    "\n",
    "def recall_at_k(actual, predicted, k=10):\n",
    "    \"\"\"Recall@K\"\"\"\n",
    "    actual_set = set(actual)\n",
    "    predicted_set = set(predicted[:k])\n",
    "    return len(actual_set & predicted_set) / float(len(actual_set)) if actual_set else 0.0\n",
    "\n",
    "\n",
    "def ndcg_at_k(actual, predicted, k=10):\n",
    "    \"\"\"Normalized Discounted Cumulative Gain@K\"\"\"\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    predicted = predicted[:k]\n",
    "    dcg = 0.0\n",
    "    for idx, item in enumerate(predicted):\n",
    "        if item in actual:\n",
    "            dcg += 1.0 / np.log2(idx + 2)\n",
    "    ideal_dcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(actual), k)))\n",
    "    return dcg / ideal_dcg if ideal_dcg > 0 else 0.0\n",
    "\n",
    "\n",
    "def average_precision_at_k(actual, predicted, k=10):\n",
    "    \"\"\"Mean Average Precision@K\"\"\"\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    predicted = predicted[:k]\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    return score / min(len(actual), k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282977b7bb47eec5",
   "metadata": {},
   "source": [
    "## 2.1 Rating Prediction Task Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea3e14bcc869d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We computed this when we optimized hyperparameters earlier but here we go again\n",
    "# RMSE of content-based, user-based, item-based, and matrix factorization model as well as the hybrid model\n",
    "\n",
    "\n",
    "if rmse_content_based and rmse_user_based and rmse_item_based and rmse_mf and rmse_hybrid:\n",
    "    print(\"Skipping evaluation, already computed.\")\n",
    "    print(f\"Content-based RMSE: {rmse_content_based}\")\n",
    "    print(f\"User-based RMSE: {rmse_user_based}\")\n",
    "    print(f\"Item-based RMSE: {rmse_item_based}\")\n",
    "    print(f\"Matrix Factorization RMSE: {rmse_mf}\")  \n",
    "    print(f\"Hybrid RMSE: {rmse_hybrid}\")\n",
    "\n",
    "else:\n",
    "    print(\"Running Content rating based evaluations on test data...\")\n",
    "    rmse_content_based = evaluate_rating_prediction_content_based(\n",
    "                train_data,\n",
    "                test_data,\n",
    "                \"full\",\n",
    "                \"weighted_avg\")\n",
    "\n",
    "    print(\"Running user based rating evaluations on test data...\")\n",
    "    rmse_user_based = evaluate_rating_prediction_user_based(train_data, test_data, user_similarity_matrix, 70)\n",
    "\n",
    "    print(\"Running item based rating evaluations on test data...\")\n",
    "    rmse_item_based = evaluate_rating_prediction_item_based(train_data, test_data, item_similarity_matrix, 20)\n",
    "\n",
    "    print(\"Running matrix factorization rating evaluations on test data...\")\n",
    "    rmse_mf = mf_model.evaluate(test_data)\n",
    "\n",
    "    print(\"Running hybrid rating evaluations on test data...\")\n",
    "    rmse_hybrid = evaluate_hybrid_models(\n",
    "        test_data,\n",
    "        train_data,\n",
    "        item_similarity_matrix,\n",
    "        user_similarity_matrix,\n",
    "        mf_model,\n",
    "        hybrid_reg_scaled,\n",
    "        bpr_model,\n",
    "        k_item=20,\n",
    "        k_user=50,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    print(type(rmse_content_based), rmse_content_based)\n",
    "    print(type(rmse_user_based), rmse_user_based)\n",
    "    print(type(rmse_item_based), rmse_item_based)\n",
    "    print(type(rmse_mf), rmse_mf)\n",
    "    print(type(rmse_hybrid), rmse_hybrid)\n",
    "\n",
    "    # --- Final Comparison ---\n",
    "    print(\"\\n--- Model RMSE Comparison ---\")\n",
    "    results = {\n",
    "        \"Content-Based\": rmse_content_based,\n",
    "        \"User-Based KNN\": rmse_user_based,\n",
    "        \"Item-Based KNN\": rmse_item_based,\n",
    "        \"Matrix Factorization\": rmse_mf,\n",
    "        \"Hybrid (Scaled)\": rmse_hybrid,\n",
    "    }\n",
    "    results_df = pd.DataFrame(list(results.items()), columns=[\"Model\", \"RMSE\"])\n",
    "    results_df = results_df.sort_values(\"RMSE\").set_index(\"Model\")\n",
    "    display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b725118eec976c",
   "metadata": {},
   "source": [
    "## 2.2 Ranking Prediction Task Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca2a94c73976400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_models_ranking(\n",
    "    train_data: pd.DataFrame,\n",
    "    test_data: pd.DataFrame,\n",
    "    user_similarity_matrix: pd.DataFrame,\n",
    "    item_similarity_matrix: pd.DataFrame,\n",
    "    mf_model,  \n",
    "    bpr_model,\n",
    "    k: int = 10,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluates all five models against the test data.\n",
    "    Model objects are passed as direct parameters.\n",
    "    Hyperparameters (k_user, k_item, etc.) are hardcoded inside.\n",
    "    \"\"\"\n",
    "\n",
    "    test_users = test_data[\"user_id\"].unique()\n",
    "\n",
    "    # Data structures to hold results\n",
    "    all_ground_truths = []\n",
    "\n",
    "    # Define the models we are testing\n",
    "    model_names = [\n",
    "        \"Content-Based\",\n",
    "        \"User-Based KNN\",\n",
    "        \"Item-Based KNN\",\n",
    "        \"Matrix Factorization\",\n",
    "        \"BPR Model\",\n",
    "    ]\n",
    "\n",
    "    all_recommendations = {model_name: [] for model_name in model_names}\n",
    "\n",
    "    for user_id in tqdm(test_users, desc=\"Evaluating all models via ranking task\"):\n",
    "\n",
    "        # Get all the items that the user has rated 4.0 or higher in the test set\n",
    "        gt = list(test_data[(test_data['user_id'] == user_id) & (test_data['rating'] >= 4.0)]['item_id'])\n",
    "\n",
    "        if not gt:\n",
    "            continue\n",
    "\n",
    "        all_ground_truths.append(gt)\n",
    "\n",
    "        # Get recommendations from each one of the model\n",
    "\n",
    "        # --- Content-Based ---\n",
    "        recs_cb = recommend_topk_content_based(\n",
    "            train_data=train_data,\n",
    "            target_user=user_id,\n",
    "            k=k,\n",
    "            content_type=\"full\",  \n",
    "            aggregation_method=\"weighted_avg\", \n",
    "        )\n",
    "\n",
    "        recs_cb_ids = [item_id for item_id, score in recs_cb]\n",
    "\n",
    "        # print(f\"User {user_id} Content-Based Recs: {recs_cb[:5]}\")\n",
    "        all_recommendations[\"Content-Based\"].append(recs_cb_ids)\n",
    "\n",
    "        # --- User-Based KNN ---\n",
    "        recs_ub = recommend_topk_user_based(\n",
    "            train_data=train_data,\n",
    "            target_user=user_id,\n",
    "            k=k,\n",
    "            user_similarity_matrix=user_similarity_matrix,\n",
    "            k_user=50,  \n",
    "        )\n",
    "\n",
    "        recs_ub_ids = [item_id for item_id, score in recs_ub]\n",
    "\n",
    "        # print(f\"User {user_id} User-Based KNN Recs: {recs_ub[:5]}\")\n",
    "        all_recommendations[\"User-Based KNN\"].append(recs_ub_ids)\n",
    "\n",
    "        # --- Item-Based KNN ---\n",
    "        recs_ib = recommend_topk_item_based(\n",
    "            train_data=train_data,\n",
    "            target_user=user_id,\n",
    "            k=k,\n",
    "            item_similarity_matrix=item_similarity_matrix,\n",
    "            k_item=20,  \n",
    "        )\n",
    "\n",
    "        recs_ib_ids = [item_id for item_id, score in recs_ib]\n",
    "        # print(f\"User {user_id} Item-Based KNN Recs: {recs_ib[:5]}\")\n",
    "        all_recommendations[\"Item-Based KNN\"].append(recs_ib_ids)\n",
    "\n",
    "        # --- Matrix Factorization ---\n",
    "        recs_mf = recommend_topk_mf(\n",
    "            mf_trainer=mf_model, train_data=train_data, target_user=user_id, k=k\n",
    "        )\n",
    "        # print(f\"User {user_id} Matrix Factorization Recs: {recs_mf[:5]}\")\n",
    "        all_recommendations[\"Matrix Factorization\"].append(recs_mf)\n",
    "\n",
    "        # --- BPR Model ---\n",
    "        recs_bpr = bpr_model.recommend(target_user, train_data, k)\n",
    "        # print(f\"User {user_id} BPR Model Recs: {recs_bpr[:5]}\")\n",
    "\n",
    "        recs_bpr_ids = [item_id for item_id, score in recs_bpr]\n",
    "        all_recommendations[\"BPR Model\"].append(recs_bpr_ids)\n",
    "\n",
    "    print(\"\\nCalculating final metrics...\")\n",
    "\n",
    "    final_results = {}\n",
    "    for model_name, rec_lists in all_recommendations.items():\n",
    "        if not rec_lists:\n",
    "            print(f\"Warning: No recommendations generated for {model_name}\")\n",
    "            continue\n",
    "\n",
    "        prec = Precision(all_ground_truths, rec_lists)\n",
    "        rec = Recall(all_ground_truths, rec_lists)\n",
    "        ndcg = NDCG(all_ground_truths, rec_lists)\n",
    "\n",
    "        final_results[model_name] = {\n",
    "            \"Precision@K\": prec,\n",
    "            \"Recall@K\": rec,\n",
    "            \"NDCG@K\": ndcg,\n",
    "        }\n",
    "\n",
    "    results_df = pd.DataFrame.from_dict(final_results, orient=\"index\")\n",
    "    results_df = results_df.sort_values(\"NDCG@K\", ascending=False)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599339693e67537",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_results = evaluate_all_models_ranking(\n",
    "    train_data,\n",
    "    test_data,\n",
    "    user_similarity_matrix=user_similarity_matrix,\n",
    "    item_similarity_matrix=item_similarity_matrix,\n",
    "    mf_model=mf_model,\n",
    "    bpr_model=bpr_model,\n",
    "    k=10,\n",
    ")\n",
    "\n",
    "# Display the final comparison table\n",
    "print(\"\\n--- Final Model Comparison (Ranking Function) ---\")\n",
    "display(all_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f3a3e5-adef-4144-b7ad-f5f55696972d",
   "metadata": {
    "id": "a7f3a3e5-adef-4144-b7ad-f5f55696972d"
   },
   "source": [
    "# Task 3) Implement baselines for both rating prediction and ranking tasks, and perform experiments with those baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5710b4-8bae-42f0-8990-41e7cec7433f",
   "metadata": {
    "id": "fc5710b4-8bae-42f0-8990-41e7cec7433f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e10661af-16f3-41f1-b09a-0307f70c344f",
   "metadata": {
    "id": "e10661af-16f3-41f1-b09a-0307f70c344f"
   },
   "source": [
    "# Task 4) Analysis of recommendation models. Analyzing the coefficients of hybrid model and the success of recommendation models for different users' groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71edb26-463c-451a-b146-76cfb304fb2a",
   "metadata": {},
   "source": [
    "## 4.1 Coefficients Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96687459-8d2c-4531-8fa5-6e66726a80ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_rating_coeff, hybrid_ranking_coeff = coefficients_scaled, hybrid_ranking_weights_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcd9d2d-9350-4381-9f36-4f46f8a0a010",
   "metadata": {},
   "source": [
    "### 4.1.1 Rating predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31ceb1e44542807",
   "metadata": {
    "id": "23377855-cc19-4b06-a497-712a892ae3f5"
   },
   "source": [
    "> We noticed that if we add BPR to the hybrid model, its coefficient is very small (close to zero). This is confirms the fact that BPR is optimized for ranking, not rating prediction, so its predicted scores do not align well with actual ratings. Therefore, the regression model assigns it a low weight when predicting ratings. Also the coefficient values do mirror the RMSE values:\n",
    "- MF (Matrix Factorization): Best RMSE $\\approx 0.96$\n",
    "- UserKNN: Best RMSE $\\approx 1.01$\n",
    "- ItemKNN: Best RMSE $\\approx 1.14$\n",
    "- CB (Content-Based): Best RMSE $\\approx 1.17$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d90742-7fc9-46b4-8e36-4f56740a8c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_rating_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5cb9a8-2ce0-4456-9acc-d7369ed5d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4), dpi=200)\n",
    "sns.barplot(data=hybrid_rating_coeff, x='Component', y='Alpha (Weight)', palette='coolwarm')\n",
    "plt.title('Hybrid Model Component Importance for Rating Prediction Task')\n",
    "plt.ylabel('Weight (Œ±)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcded4ec-c21d-4213-907c-32b7b823827d",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "- MF (0.36) dominates, as expected ‚Äî it‚Äôs excellent at modeling user‚Äìitem rating patterns.\n",
    "- UserKNN (0.31) still provides complementary information about local neighborhood similarities.\n",
    "- ItemKNN contributes almost nothing ‚Äî likely redundant with MF and user-based CF.\n",
    "- Content-based is negligible ‚Äî explicit ratings are driven more by collaborative patterns than by item content.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "For rating prediction, the hybrid model relies mainly on MF and UserKNN, with MF being slightly more influential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77510246-87d1-4ed6-b2bf-57ac147c6c10",
   "metadata": {},
   "source": [
    "### 4.1.2 Ranking predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fb2ba0-30b9-4049-81a3-d7d5a1e73f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_ranking_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a8e86c-9bca-4a68-9167-c292f4952ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4), dpi=200)\n",
    "sns.barplot(data=hybrid_ranking_coeff, x='Component', y='Weight (Œ±)', palette='coolwarm')\n",
    "plt.title('Hybrid Model Component Importance for Ranking Task')\n",
    "plt.ylabel('Weight (Œ±)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbee96e-d6f3-496c-a81b-8273ab22ff8d",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "- BPR (0.46) has the highest coefficient ‚Äî consistent with its design as a ranking-optimized model (pairwise loss).\n",
    "- MF (0.26) and UserKNN (0.29) provide secondary signals; both capture collaborative structure.\n",
    "- Content-based (0.11) and ItemKNN (0.09) offer additional diversity (especially helpful for cold-start and diversity).\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "- For ranking tasks, the hybrid model relies primarily on BPR, followed by UserKNN and MF.\n",
    "- Content-based and ItemKNN models make smaller but complementary contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8727a5-876b-4a8e-bb82-ec327da2f4b7",
   "metadata": {},
   "source": [
    "## 4.2 User Group Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bdd039-6d66-4963-ab22-c6f912dad2dc",
   "metadata": {},
   "source": [
    "Group users by **activity level** (user behaviour length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c6b1d0-fd79-4aa7-ab35-4eb98cbca575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_by_user_group_ranking(\n",
    "    train_data,\n",
    "    test_data,\n",
    "    user_similarity_matrix,\n",
    "    item_similarity_matrix,\n",
    "    mf_model,\n",
    "    bpr_model,\n",
    "    device=\"cpu\",\n",
    "    k_recall=10,\n",
    "    user_bins=[0, 10, 50, 200, np.inf],\n",
    "    k_user=70,\n",
    "    k_item=20,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate ranking metrics (Precision@K, Recall@K, NDCG@K, MAP@K)\n",
    "    for CB, UserKNN, ItemKNN, MF, and BPR models by user group.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # 1 Split users by activity\n",
    "    user_counts = train_data.groupby(\"user_id\").size()\n",
    "    user_groups = pd.cut(\n",
    "        user_counts,\n",
    "        bins=user_bins,\n",
    "        labels=[\"cold\", \"light\", \"medium\", \"heavy\"]\n",
    "    )\n",
    "\n",
    "    global_mean_rating = train_data[\"rating\"].mean()\n",
    "    all_items = test_data[\"item_id\"].unique()\n",
    "\n",
    "    # 2 Evaluate per group\n",
    "    for group_name, group_users in user_groups.groupby(user_groups):\n",
    "        group_user_ids = group_users.index.tolist()\n",
    "        subset = test_data[test_data[\"user_id\"].isin(group_user_ids)]\n",
    "        if subset.empty:\n",
    "            continue\n",
    "\n",
    "        group_result = {\"group\": group_name}\n",
    "\n",
    "        # 3 Evaluate each model\n",
    "        for model_name in [\"cb\", \"userknn\", \"itemknn\", \"mf\", \"bpr\"]:\n",
    "            precisions, recalls, ndcgs, maps = [], [], [], []\n",
    "\n",
    "            for user_id in tqdm(group_user_ids, desc=f\"Group {group_name} - {model_name}\", leave=False):\n",
    "                true_items = test_data[test_data[\"user_id\"] == user_id][\"item_id\"].tolist()\n",
    "                if len(true_items) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Candidate items (exclude those seen in train)\n",
    "                user_train_items = train_data[train_data[\"user_id\"] == user_id][\"item_id\"].tolist()\n",
    "                candidates = [i for i in all_items if i not in user_train_items]\n",
    "\n",
    "                scores = []\n",
    "                for item_id in candidates:\n",
    "                    try:\n",
    "                        if model_name == \"cb\":\n",
    "                            score = get_user_item_prediction_content_based(\n",
    "                                train_data, user_id, item_id, \"full\", \"weighted_avg\"\n",
    "                            )\n",
    "                        elif model_name == \"userknn\":\n",
    "                            score = predict_rating_user_based(\n",
    "                                train_data, user_similarity_matrix,\n",
    "                                user_id, item_id, global_mean_rating, k=k_user\n",
    "                            )\n",
    "                        elif model_name == \"itemknn\":\n",
    "                            score = predict_rating_item_based(\n",
    "                                train_data, item_similarity_matrix,\n",
    "                                user_id, item_id, k=k_item\n",
    "                            )\n",
    "                        elif model_name == \"mf\":\n",
    "                            score = mf_model.predict(user_id, item_id)\n",
    "                        elif model_name == \"bpr\":\n",
    "                            with torch.no_grad():\n",
    "                                score = bpr_model.model.predict(\n",
    "                                    torch.LongTensor([user_id - 1]).to(device),\n",
    "                                    torch.LongTensor([item_id - 1]).to(device)\n",
    "                                ).item()\n",
    "                        else:\n",
    "                            continue\n",
    "                        scores.append((item_id, score))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "                if len(scores) == 0:\n",
    "                    continue\n",
    "\n",
    "                ranked_items = [i for i, _ in sorted(scores, key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "                precisions.append(precision_at_k(true_items, ranked_items, k=k_recall))\n",
    "                recalls.append(recall_at_k(true_items, ranked_items, k=k_recall))\n",
    "                ndcgs.append(ndcg_at_k(true_items, ranked_items, k=k_recall))\n",
    "                maps.append(average_precision_at_k(true_items, ranked_items, k=k_recall))\n",
    "\n",
    "            # Aggregate metrics\n",
    "            group_result[f\"{model_name}_Precision@{k_recall}\"] = np.nanmean(precisions)\n",
    "            group_result[f\"{model_name}_Recall@{k_recall}\"] = np.nanmean(recalls)\n",
    "            group_result[f\"{model_name}_NDCG@{k_recall}\"] = np.nanmean(ndcgs)\n",
    "            group_result[f\"{model_name}_MAP@{k_recall}\"] = np.nanmean(maps)\n",
    "\n",
    "        results.append(group_result)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"Multi-metric ranking evaluation completed by user group.\")\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59763ab-c0dc-4eef-854f-c0e9f04cc468",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"====== MORE THAN 1 HOUR IS NEEDED TO RUN THE CODE BELOW ======\"\"\"\n",
    "# results_multi_metric = evaluate_by_user_group_ranking(\n",
    "#     train_data=train_data,\n",
    "#     test_data=test_data,\n",
    "#     user_similarity_matrix=user_similarity_matrix,\n",
    "#     item_similarity_matrix=item_similarity_matrix,\n",
    "#     mf_model=mf_model,\n",
    "#     bpr_model=bpr_model,\n",
    "#     device=device,\n",
    "#     k_recall=10\n",
    "# )\n",
    "\n",
    "# results_multi_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c968df5b-3f25-407e-b202-d9c85af4c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_multi_metric.to_csv(os.path.join(OUTPUTS_PATH, 'user_group_analysis.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3dde29-6082-4032-a0f6-c07bba4c10dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"====== LOAD THE RESULTS INSTEAD ======\"\"\"\n",
    "results_multi_metric = pd.read_csv(os.path.join(OUTPUTS_PATH, 'user_group_analysis.csv'))\n",
    "results_multi_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea3bd3-652c-4a10-9813-7ecbb0792157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ranking_metrics_by_group(results_df, k=10, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot grouped bar charts for Precision@K, Recall@K, NDCG@K, and MAP@K.\n",
    "    \"\"\"\n",
    "    metrics = [f\"Precision@{k}\", f\"Recall@{k}\", f\"NDCG@{k}\", f\"MAP@{k}\"]\n",
    "    models = [\"cb\", \"userknn\", \"itemknn\", \"mf\", \"bpr\"]\n",
    "\n",
    "    # Melt dataframe for plotting\n",
    "    plot_data = []\n",
    "    for _, row in results_df.iterrows():\n",
    "        group = row[\"group\"]\n",
    "        for model in models:\n",
    "            for metric in metrics:\n",
    "                col = f\"{model}_{metric}\"\n",
    "                if col in row:\n",
    "                    plot_data.append({\n",
    "                        \"Group\": group,\n",
    "                        \"Model\": model.upper(),\n",
    "                        \"Metric\": metric.replace(f\"@{k}\", \"\"),\n",
    "                        \"Value\": row[col]\n",
    "                    })\n",
    "\n",
    "    df_plot = pd.DataFrame(plot_data)\n",
    "\n",
    "    # Seaborn style setup\n",
    "    sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "    plt.figure(figsize=(16, 10), dpi=400)\n",
    "\n",
    "    # Plot 4 metrics separately\n",
    "    for i, metric in enumerate(metrics, 1):\n",
    "        plt.subplot(2, 2, i)\n",
    "        subset = df_plot[df_plot[\"Metric\"] == metric.replace(f\"@{k}\", \"\")]\n",
    "        sns.barplot(data=subset, x=\"Group\", y=\"Value\", hue=\"Model\")\n",
    "        plt.title(metric)\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.xlabel(\"User Group\")\n",
    "        plt.legend(title=\"Model\", loc=\"upper right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"‚úÖ Plot saved to {save_path}\")\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a075bf-e0e7-4978-8e22-ba6e5ecffd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ranking_metrics_by_group(results_multi_metric, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6e6f1a-b15b-429c-a836-d3ca06fae77b",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "In this case, **most metrics (Precision, NDCG, MAP)** show that recommendation accuracy increases with user activity ‚Äî\n",
    "**heavy users > medium > light > cold**,\n",
    "meaning models perform best for users with more interactions, since their preferences are easier to learn.\n",
    "\n",
    "However, **Recall** shows the opposite trend (**cold > light > medium > heavy**), indicating that although models make less precise predictions for cold users, they still retrieve a larger portion of relevant items ‚Äî <u>possibly because cold users have fewer total relevant items, making it easier to achieve higher recall</u>.\n",
    "\n",
    "üëâ **Summary:**\n",
    "\n",
    "* **Best performance (overall accuracy):** Heavy users\n",
    "* **Highest recall:** Cold users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9914f603-b893-471c-9622-4437855dd8fa",
   "metadata": {
    "id": "9914f603-b893-471c-9622-4437855dd8fa"
   },
   "source": [
    "# Task 5) Evaluation of beyond accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZsPBgQ1Hld5U",
   "metadata": {
    "id": "ZsPBgQ1Hld5U"
   },
   "source": [
    "## Diversity (intra-list diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sxev9kGZ91Lt",
   "metadata": {
    "id": "sxev9kGZ91Lt"
   },
   "source": [
    "**Intra-List Diversity (ILD)** quantifies how *different* the items within a single user‚Äôs recommendation list are from each other.\n",
    "\n",
    "Formally:\n",
    "[\n",
    "ILD(u) = \\frac{1}{|L_u|(|L_u|-1)} \\sum_{i \\ne j} (1 - \\text{sim}(i,j))\n",
    "]\n",
    "where `sim(i,j)` is cosine similarity between item embeddings (here, BERT item representations).\n",
    "Then we average across users to get the final ILD score.\n",
    "\n",
    "* **High ILD (close to 1)** ‚Üí items are *very dissimilar* ‚Üí *diverse recommendations* (e.g., a mix of action, romance, and comedy movies).\n",
    "* **Low ILD (close to 0)** ‚Üí items are *very similar* ‚Üí *homogeneous recommendations* (e.g., 10 action movies with similar plots).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heLlPj9woUxB",
   "metadata": {
    "id": "heLlPj9woUxB"
   },
   "source": [
    "### Define distance function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MbC5yI8AoaV2",
   "metadata": {
    "id": "MbC5yI8AoaV2"
   },
   "source": [
    "> We'll use cosine distance based on item content embeddings (BERT or title+genre)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lKLoeMLIoTzj",
   "metadata": {
    "id": "lKLoeMLIoTzj"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity as cosine_sim_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_intra_list_diversity(rec_lists, item_embeddings):\n",
    "    \"\"\"\n",
    "    Compute average ILD across all users.\n",
    "\n",
    "    Args:\n",
    "        rec_lists (dict): {user_id: [item_ids]} ‚Äì recommended items per user\n",
    "        item_embeddings (np.ndarray): matrix of item embeddings (indexed by item_id - 1)\n",
    "    Returns:\n",
    "        float: average ILD score\n",
    "    \"\"\"\n",
    "    ild_scores = []\n",
    "    for user, rec_items in rec_lists.items():\n",
    "        if len(rec_items) < 2:\n",
    "            continue  # skip too-short lists\n",
    "        embs = item_embeddings[[i - 1 for i in rec_items]]\n",
    "        sim_matrix = cosine_sim_matrix(embs)\n",
    "        # convert to distance\n",
    "        dist_matrix = 1 - sim_matrix\n",
    "        n = len(rec_items)\n",
    "        ild = dist_matrix.sum() / (n * (n - 1))\n",
    "        ild_scores.append(ild)\n",
    "    return np.mean(ild_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fKay7rg6ojS4",
   "metadata": {
    "id": "fKay7rg6ojS4"
   },
   "source": [
    "### Generate recommendation lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YW8PeKyaomnJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329176,
     "status": "ok",
     "timestamp": 1761058255776,
     "user": {
      "displayName": "Elena Mihalache",
      "userId": "05431839471568078681"
     },
     "user_tz": -120
    },
    "id": "YW8PeKyaomnJ",
    "outputId": "8bf62072-65dd-448c-ca21-2fb8522285e9"
   },
   "outputs": [],
   "source": [
    "# Generate Top-10 lists for a random sample of users\n",
    "sample_users = np.random.choice(train_data['user_id'].unique(), size=10, replace=False)\n",
    "\n",
    "# topk_cb, topk_userknn, topk_itemknn, topk_mf, topk_bpr, topk_hybrid = {}, {}, {}, {}, {}, {}\n",
    "topk_itemknn = {}\n",
    "topk_hybrid = {}\n",
    "\n",
    "for user in tqdm(sample_users):\n",
    "    # topk_cb[user] = recommend_topk_item_based(train_data, item_similarity_matrix, user, k=10)\n",
    "    # topk_userknn[user] = [i for i, _ in recommend_topk_user_based(train_data, user_similarity_matrix, user, k=10)]\n",
    "    topk_itemknn[user] = [i for i, _ in recommend_topk_item_based(train_data, item_similarity_matrix, user, k_item=20, k=10)]\n",
    "    # topk_mf[user] = [i for i, _ in recommend_topk_MF(mf_model, train_data, user, k=10)]\n",
    "    # topk_bpr[user] = [i for i, _ in bpr_model.recommend(user, train_data=train_data, top_k=10)]\n",
    "    topk_hybrid[user] = [\n",
    "    i for i, _ in recommend_topk_hybrid_ranking(\n",
    "        user,\n",
    "        hybrid_reg_scaled,\n",
    "        train_data,\n",
    "        item_similarity_matrix,\n",
    "        user_similarity_matrix,\n",
    "        mf_model,\n",
    "        bpr_model,\n",
    "        k_item=20, \n",
    "        k_user=70, \n",
    "        top_k=10, \n",
    "        device=device\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IuOQinJNqNlq",
   "metadata": {
    "id": "IuOQinJNqNlq"
   },
   "source": [
    "### Compute ILD per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8UQY8IDRqPh6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1761058255803,
     "user": {
      "displayName": "Elena Mihalache",
      "userId": "05431839471568078681"
     },
     "user_tz": -120
    },
    "id": "8UQY8IDRqPh6",
    "outputId": "e9df4b63-f619-486b-dee0-0bd80385e221"
   },
   "outputs": [],
   "source": [
    "emb_to_use = item_emb_full  # or item_emb_titlegenres\n",
    "\n",
    "# ild_cb = compute_intra_list_diversity(topk_cb, emb_to_use)\n",
    "# ild_userknn = compute_intra_list_diversity(topk_userknn, emb_to_use)\n",
    "ild_itemknn = compute_intra_list_diversity(topk_itemknn, emb_to_use)\n",
    "# ild_mf = compute_intra_list_diversity(topk_mf, emb_to_use)\n",
    "# ild_bpr = compute_intra_list_diversity(topk_bpr, emb_to_use)\n",
    "ild_hybrid = compute_intra_list_diversity(topk_hybrid, emb_to_use)\n",
    "\n",
    "results_ild = pd.DataFrame({\n",
    "    'Model': ['CB', 'UserKNN', 'ItemKNN', 'MF', 'BPR', 'Hybrid'],\n",
    "    'Intra-List Diversity': [None, None, ild_itemknn, None, None, ild_hybrid]\n",
    "})\n",
    "display(results_ild)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "slwqF3Z1qatT",
   "metadata": {
    "id": "slwqF3Z1qatT"
   },
   "source": [
    "### Visualize diversity comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CjM8fcG5qeOb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 176,
     "status": "ok",
     "timestamp": 1761058256000,
     "user": {
      "displayName": "Elena Mihalache",
      "userId": "05431839471568078681"
     },
     "user_tz": -120
    },
    "id": "CjM8fcG5qeOb",
    "outputId": "ec4e01f8-fa7a-4d34-ea90-73fb55c9e67e"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=results_ild, x='Model', y='Intra-List Diversity', palette='viridis')\n",
    "plt.title('Intra-List Diversity (ILD) Comparison Across Models')\n",
    "plt.ylabel('Average ILD')\n",
    "plt.xlabel('Recommendation Model')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tiJvJBQKlkO0",
   "metadata": {
    "id": "tiJvJBQKlkO0"
   },
   "source": [
    "## Novelty (surprisal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EbTGPvqn-6dC",
   "metadata": {
    "id": "EbTGPvqn-6dC"
   },
   "source": [
    "Measure each item‚Äôs novelty as:\n",
    "\n",
    "[\n",
    "\\text{novelty}(i)=-\\log_2 p(i)\n",
    "\\quad\\text{with}\\quad\n",
    "p(i)=\\frac{#\\text{interactions on }i}{\\text{total interactions}}\n",
    "]\n",
    "\n",
    "A user‚Äôs novelty is the **mean** surprisal of the items in their recommended list, and finally average across users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AfYn_X7p_DSS",
   "metadata": {
    "id": "AfYn_X7p_DSS"
   },
   "source": [
    "### Item popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WLD3XGJ6_Vmj",
   "metadata": {
    "id": "WLD3XGJ6_Vmj"
   },
   "outputs": [],
   "source": [
    "def build_item_popularity(train_df):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      pop: pd.Series indexed by item_id with p(i) in [0,1]\n",
    "    \"\"\"\n",
    "    item_counts = train_df['item_id'].value_counts().sort_index()\n",
    "    total = item_counts.sum()\n",
    "    pop = item_counts / total\n",
    "    # also ensure every known item appears (even if 0 in train subset)\n",
    "    all_items = train_df['item_id'].unique()\n",
    "    pop = pop.reindex(sorted(all_items), fill_value=0.0)\n",
    "    return pop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Jquo4Ch1_Pwq",
   "metadata": {
    "id": "Jquo4Ch1_Pwq"
   },
   "source": [
    "### Item surprisal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jaFpNo3e_ZiG",
   "metadata": {
    "id": "jaFpNo3e_ZiG"
   },
   "outputs": [],
   "source": [
    "def build_item_surprisal(pop, eps=1e-12):\n",
    "    \"\"\"\n",
    "    novelty(i) = -log2(pop(i) + eps)  (in bits)\n",
    "    \"\"\"\n",
    "    return -np.log2(pop + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GtdMPu42_cTc",
   "metadata": {
    "id": "GtdMPu42_cTc"
   },
   "source": [
    "### Example build from training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zwvgIz76_h2T",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1761058256102,
     "user": {
      "displayName": "Elena Mihalache",
      "userId": "05431839471568078681"
     },
     "user_tz": -120
    },
    "id": "zwvgIz76_h2T",
    "outputId": "82967bf5-6274-490a-ce9a-6572daafc61b"
   },
   "outputs": [],
   "source": [
    "item_pop = build_item_popularity(train_data)\n",
    "item_surprisal = build_item_surprisal(item_pop)\n",
    "\n",
    "print(\"Example surprisal (higher = more novel):\")\n",
    "display(item_surprisal.sample(5).to_frame(\"novelty_bits\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3iRpLzLhlnBb",
   "metadata": {
    "id": "3iRpLzLhlnBb"
   },
   "source": [
    "## Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hSllKW6Jlpv4",
   "metadata": {
    "id": "hSllKW6Jlpv4"
   },
   "source": [
    "## Fairness metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5334ff-e866-4f6e-bc68-9f4358e58bfc",
   "metadata": {
    "id": "4c4a0e4b-adbe-4673-82f8-a75761666fd1"
   },
   "source": [
    "### User Side Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e237cf71-0b6c-45fc-a52f-ef9e8f8bbc67",
   "metadata": {},
   "source": [
    "#### Group Recommendation Unfairness (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7624be-9b96-4629-9409-4655bc91c9ed",
   "metadata": {},
   "source": [
    "### Item Side Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6519876-39b9-434b-84e1-dcd05470b3ae",
   "metadata": {},
   "source": [
    "#### Demographic Parity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5555d0-bb3c-4fb5-af72-8d79d21aa060",
   "metadata": {},
   "source": [
    "#### Disparate treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b396f70f-af80-4e78-9178-85c17de8626c",
   "metadata": {},
   "source": [
    "#### Disparate Impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d712bc-63dc-4701-993a-9ed42191740d",
   "metadata": {},
   "source": [
    "# Bibliography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3fc9a7-2a7e-4cf2-868f-7e15a16b00cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
